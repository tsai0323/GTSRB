# ============================================================================
# GTSRB Traffic Sign Classification - Complete 4 Experiments Comparison
# äº¤é€šæ¨™èªŒè¾¨è­˜ï¼šå®Œæ•´å››å¯¦é©—æ¯”è¼ƒ
# Baseline vs Normalized vs Augmented vs Augmented+BatchNorm
# ============================================================================

print("=" * 80)
print("ğŸ”¬ GTSRB äº¤é€šæ¨™èªŒè¾¨è­˜ - å››å¯¦é©—å®Œæ•´æ¯”è¼ƒ")
print("=" * 80)
print("å¯¦é©—æ¶æ§‹:")
print("  Baseline: ç„¡æ­£è¦åŒ–")
print("  å¯¦é©— A: æ­£è¦åŒ– (Min-Max)")
print("  å¯¦é©— B: æ­£è¦åŒ– + è³‡æ–™æ“´å¢")
print("  å¯¦é©— C: æ­£è¦åŒ– + è³‡æ–™æ“´å¢ + Batch Normalization")
print("=" * 80)

# ============================================================================
# Section 1: ç’°å¢ƒè¨­å®šèˆ‡å¥—ä»¶è¼‰å…¥
# ============================================================================

print("\nğŸ“¦ è¼‰å…¥å¥—ä»¶ä¸­...")

# æ›è¼‰ Google Drive
from google.colab import drive
drive.mount('/content/drive')

# åŸºç¤å¥—ä»¶
import numpy as np
import pandas as pd
import tensorflow as tf
import os
import time
import pickle

# å½±åƒè™•ç†
from PIL import Image

# ç¹ªåœ–å¥—ä»¶
import matplotlib.pyplot as plt
import seaborn as sns

# æ¨¡å‹ç›¸é—œ
from sklearn.model_selection import train_test_split
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPool2D, Dense, Flatten, Dropout, BatchNormalization
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.callbacks import EarlyStopping

# è¨­å®šé¡¯ç¤ºé¢¨æ ¼
plt.style.use('seaborn-v0_8-darkgrid')
sns.set_palette("husl")

# è¨­å®šäº‚æ•¸ç¨®å­ï¼Œç¢ºä¿çµæœå¯é‡ç¾
np.random.seed(42)
tf.random.set_seed(42)

print("âœ… å¥—ä»¶è¼‰å…¥å®Œæˆ")
print(f"TensorFlow ç‰ˆæœ¬: {tf.__version__}")
print(f"GPU å¯ç”¨: {tf.config.list_physical_devices('GPU')}")

# ============================================================================
# Section 2: è³‡æ–™è¼‰å…¥
# ============================================================================

print("\n" + "=" * 80)
print("ğŸ“‚ Section 2: è³‡æ–™è¼‰å…¥")
print("=" * 80)

# è·¯å¾‘è¨­å®š
classes = 43  # GTSRB è³‡æ–™é›†æœ‰ 43 å€‹é¡åˆ¥
train_dir = '/content/drive/MyDrive/Colab Notebooks/GTSRB/archive/Train'
save_dir = '/content/drive/MyDrive/Colab Notebooks/GTSRB/'
data_path = os.path.join(save_dir, 'X_data.npy')
label_path = os.path.join(save_dir, 'y_labels.npy')

print("ğŸ“‚ è³‡æ–™è·¯å¾‘è¨­å®š:")
print(f"   è¨“ç·´è³‡æ–™å¤¾: {train_dir}")
print(f"   å¿«å–æª”æ¡ˆ: {data_path}")

# è®€å–å½±åƒèˆ‡æ¨™ç±¤ï¼ˆä½¿ç”¨å¿«å–ï¼‰
if os.path.exists(data_path) and os.path.exists(label_path):
    print("\nâœ… åµæ¸¬åˆ°å¿«å–æª”æ¡ˆï¼Œç›´æ¥è¼‰å…¥ä¸­...")
    data = np.load(data_path)
    labels = np.load(label_path)
    print(f"è¼‰å…¥å®Œæˆ:")
    print(f"  X_data.shape = {data.shape}")
    print(f"  y_labels.shape = {labels.shape}")
else:
    print("\nâš™ï¸ æœªåµæ¸¬åˆ°å¿«å–æª”æ¡ˆï¼Œé–‹å§‹è®€å–å½±åƒä¸¦å»ºç«‹è³‡æ–™é›†...")
    data = []
    labels = []

    # ä¾åºè®€å–æ¯ä¸€å€‹é¡åˆ¥è³‡æ–™å¤¾ (0 åˆ° 42)
    for i in range(classes):
        path = os.path.join(train_dir, str(i))
        images = os.listdir(path)

        for img_name in images:
            try:
                image = Image.open(os.path.join(path, img_name))
                image = image.resize((30, 30))
                image = np.array(image)
                data.append(image)
                labels.append(i)
            except Exception as e:
                print(f"Error loading image {img_name}: {e}")

        if (i + 1) % 10 == 0:
            print(f"å·²å®Œæˆ {i + 1}/{classes} å€‹é¡åˆ¥")

    # è½‰æ›æˆ NumPy é™£åˆ—
    data = np.array(data)
    labels = np.array(labels)
    print(f"\nâœ… è®€å–å®Œæˆ!")
    print(f"X_data.shape = {data.shape}")
    print(f"y_labels.shape = {labels.shape}")

    # å„²å­˜æˆ .npy æª”
    np.save(data_path, data)
    np.save(label_path, labels)
    print(f"\nğŸ’¾ å·²å„²å­˜å¿«å–æª”æ¡ˆè‡³: {save_dir}")

# é¡¯ç¤ºè³‡æ–™è³‡è¨Š
print("\nğŸ“Š åŸå§‹è³‡æ–™å½¢ç‹€:")
print(f"  X (data): {data.shape}  â†’ å…± {data.shape[0]:,} å¼µå½±åƒ")
print(f"  æ¯å¼µå½±åƒ: {data.shape[1]}Ã—{data.shape[2]} åƒç´ ï¼Œ{data.shape[3]} å€‹é€šé“")
print(f"  y (labels): {labels.shape}  â†’ å…± {labels.shape[0]:,} ç­†æ¨™ç±¤")
print(f"  åƒç´ å€¼ç¯„åœ: [{data.min()}, {data.max()}]")

# ============================================================================
# Section 3: è³‡æ–™è¦–è¦ºåŒ–
# ============================================================================

print("\n" + "=" * 80)
print("ğŸ“Š Section 3: è³‡æ–™è¦–è¦ºåŒ–")
print("=" * 80)

# é¡åˆ¥åç¨±å­—å…¸
class_names = {
    0:'Speed limit (20km/h)', 1:'Speed limit (30km/h)', 2:'Speed limit (50km/h)',
    3:'Speed limit (60km/h)', 4:'Speed limit (70km/h)', 5:'Speed limit (80km/h)',
    6:'End of speed limit (80km/h)', 7:'Speed limit (100km/h)', 8:'Speed limit (120km/h)',
    9:'No passing', 10:'No passing veh over 3.5 tons', 11:'Right-of-way at intersection',
    12:'Priority road', 13:'Yield', 14:'Stop', 15:'No vehicles',
    16:'Veh > 3.5 tons prohibited', 17:'No entry', 18:'General caution',
    19:'Dangerous curve left', 20:'Dangerous curve right', 21:'Double curve',
    22:'Bumpy road', 23:'Slippery road', 24:'Road narrows on the right',
    25:'Road work', 26:'Traffic signals', 27:'Pedestrians', 28:'Children crossing',
    29:'Bicycles crossing', 30:'Beware of ice/snow', 31:'Wild animals crossing',
    32:'End speed + passing limits', 33:'Turn right ahead', 34:'Turn left ahead',
    35:'Ahead only', 36:'Go straight or right', 37:'Go straight or left',
    38:'Keep right', 39:'Keep left', 40:'Roundabout mandatory',
    41:'End of no passing', 42:'End no passing veh > 3.5 tons'
}

# 3.1 è¦–è¦ºåŒ–æ¯å€‹é¡åˆ¥çš„å½±åƒæ•¸é‡
print("\nç¹ªè£½é¡åˆ¥åˆ†å¸ƒåœ–...")
data_dic = {}
for folder in os.listdir(train_dir):
    try:
        data_dic[int(folder)] = len(os.listdir(os.path.join(train_dir, folder)))
    except:
        pass

data_df = pd.Series(data_dic).sort_index()
data_df.index = data_df.index.map(class_names)

plt.figure(figsize=(18, 8))
data_df.sort_values().plot(kind='bar', color='steelblue', edgecolor='black')
plt.xlabel('Class Name', fontsize=12, fontweight='bold')
plt.ylabel('Number of Images', fontsize=12, fontweight='bold')
plt.title('Number of Training Images per Class', fontsize=14, fontweight='bold')
plt.xticks(rotation=90)
plt.grid(axis='y', alpha=0.3)
plt.tight_layout()
plt.show()

print(f"ğŸ“Š è³‡æ–™åˆ†å¸ƒçµ±è¨ˆ:")
print(f"  æœ€å¤šå½±åƒçš„é¡åˆ¥: {data_df.idxmax()} ({data_df.max()} å¼µ)")
print(f"  æœ€å°‘å½±åƒçš„é¡åˆ¥: {data_df.idxmin()} ({data_df.min()} å¼µ)")
print(f"  å¹³å‡æ¯é¡: {data_df.mean():.0f} å¼µ")

# 3.2 é¡¯ç¤ºéš¨æ©Ÿæ¨£æœ¬å½±åƒ
print("\nç¹ªè£½éš¨æ©Ÿæ¨£æœ¬...")
fig, axes = plt.subplots(3, 8, figsize=(16, 6))
fig.suptitle('Random Sample Images from Dataset', fontsize=14, fontweight='bold')

for i, ax in enumerate(axes.flat):
    idx = np.random.randint(0, len(data))
    ax.imshow(data[idx])
    ax.set_title(f"Class {labels[idx]}", fontsize=9)
    ax.axis('off')

plt.tight_layout()
plt.show()

# ============================================================================
# Section 4: è³‡æ–™åˆ†å‰²èˆ‡æ¨™ç±¤è™•ç†
# ============================================================================

print("\n" + "=" * 80)
print("ğŸ“Š Section 4: è³‡æ–™åˆ†å‰²èˆ‡æ¨™ç±¤è™•ç†")
print("=" * 80)

# 4.1 åˆ†å‰²è¨“ç·´é›†èˆ‡æ¸¬è©¦é›† (80/20)
X_train, X_test, y_train, y_test = train_test_split(
    data,
    labels,
    test_size=0.2,
    random_state=42,
    stratify=labels
)

print("âœ… è³‡æ–™åˆ†å‰²å®Œæˆ:")
print(f"  è¨“ç·´é›†: {X_train.shape[0]:,} å¼µå½±åƒ ({X_train.shape[0]/len(data)*100:.1f}%)")
print(f"  æ¸¬è©¦é›†: {X_test.shape[0]:,} å¼µå½±åƒ ({X_test.shape[0]/len(data)*100:.1f}%)")
print(f"  è¨“ç·´é›†æ¨™ç±¤åˆ†å¸ƒ: {np.bincount(y_train).min()} ~ {np.bincount(y_train).max()} å¼µ/é¡åˆ¥")
print(f"  æ¸¬è©¦é›†æ¨™ç±¤åˆ†å¸ƒ: {np.bincount(y_test).min()} ~ {np.bincount(y_test).max()} å¼µ/é¡åˆ¥")

# 4.2 One-Hot Encoding
y_train_encoded = to_categorical(y_train, classes)
y_test_encoded = to_categorical(y_test, classes)

print("\nâœ… One-Hot Encoding å®Œæˆ:")
print(f"  y_train_encoded.shape: {y_train_encoded.shape}")
print(f"  y_test_encoded.shape: {y_test_encoded.shape}")

# ============================================================================
# Section 5: æº–å‚™å››ç¨®è³‡æ–™ç‰ˆæœ¬
# ============================================================================

print("\n" + "=" * 80)
print("ğŸ”§ Section 5: æº–å‚™å››ç¨®è³‡æ–™ç‰ˆæœ¬")
print("=" * 80)

# ç‰ˆæœ¬ 1: Baseline - ç„¡æ¨™æº–åŒ– [0, 255]
X_train_baseline = X_train.copy()
X_test_baseline = X_test.copy()

print("ğŸ“Œ ç‰ˆæœ¬ 1 - Baseline (ç„¡æ¨™æº–åŒ–):")
print(f"  ç¯„åœ: [{X_train_baseline.min()}, {X_train_baseline.max()}]")
print(f"  å‹åˆ¥: {X_train_baseline.dtype}")

# ç‰ˆæœ¬ 2: å¯¦é©— A - Min-Max Normalization [0, 1]
X_train_norm = X_train.astype('float32') / 255.0
X_test_norm = X_test.astype('float32') / 255.0

print("\nğŸ“Œ ç‰ˆæœ¬ 2 - å¯¦é©— A (Min-Max Normalization):")
print(f"  ç¯„åœ: [{X_train_norm.min():.4f}, {X_train_norm.max():.4f}]")
print(f"  å‹åˆ¥: {X_train_norm.dtype}")
print(f"  è½‰æ›å…¬å¼: X_normalized = X / 255.0")

# ç‰ˆæœ¬ 3 & 4: å¯¦é©— B & C - ä¿æŒ [0, 255] çµ¦ ImageDataGenerator
X_train_aug = X_train.copy()  # å¯¦é©— B å’Œ C å…±ç”¨
X_test_aug_norm = X_test.astype('float32') / 255.0  # æ¸¬è©¦é›†éœ€æ¨™æº–åŒ–

print("\nğŸ“Œ ç‰ˆæœ¬ 3 & 4 - å¯¦é©— B & C (Data Augmentation ç”¨):")
print(f"  è¨“ç·´é›†ç¯„åœ: [{X_train_aug.min()}, {X_train_aug.max()}] (çµ¦ ImageDataGenerator)")
print(f"  æ¸¬è©¦é›†ç¯„åœ: [{X_test_aug_norm.min():.4f}, {X_test_aug_norm.max():.4f}] (å·²æ¨™æº–åŒ–)")

# 5.1 è¦–è¦ºåŒ–æ¨™æº–åŒ–å‰å¾Œçš„å·®ç•°
print("\nç¹ªè£½æ¨™æº–åŒ–å‰å¾Œå°æ¯”åœ–...")
fig, axes = plt.subplots(2, 5, figsize=(15, 6))
fig.suptitle('Comparison: Raw vs Normalized Images', fontsize=14, fontweight='bold')

sample_indices = np.random.choice(len(X_train), 5, replace=False)

for i, idx in enumerate(sample_indices):
    # ç¬¬ä¸€åˆ—: åŸå§‹å½±åƒ
    axes[0, i].imshow(X_train_baseline[idx].astype('uint8'))
    axes[0, i].set_title(f'Raw [0-255]\nClass {y_train[idx]}', fontsize=10)
    axes[0, i].axis('off')

    # ç¬¬äºŒåˆ—: æ¨™æº–åŒ–å¾Œå½±åƒ
    axes[1, i].imshow(X_train_norm[idx])
    axes[1, i].set_title(f'Normalized [0-1]\nClass {y_train[idx]}', fontsize=10)
    axes[1, i].axis('off')

plt.tight_layout()
plt.show()

print("ğŸ’¡ è§€å¯Ÿé‡é»:")
print("  - è¦–è¦ºä¸Šå…©è€…çœ‹èµ·ä¾†ç›¸åŒ (å› ç‚ºåªæ˜¯ç¸®æ”¾æ¯”ä¾‹)")
print("  - ä½†æ•¸å€¼ç¯„åœä¸åŒæœƒå½±éŸ¿ç¥ç¶“ç¶²è·¯çš„è¨“ç·´éç¨‹")
print("  - æ¨™æº–åŒ–å¾Œçš„æ¢¯åº¦æ›´æ–°æ›´ç©©å®šã€æ”¶æ–‚æ›´å¿«")

# 5.2 è¨­å®š Data Augmentation (å¯¦é©— B & C ç”¨)
print("\nâš™ï¸ è¨­å®š Data Augmentation...")

datagen = ImageDataGenerator(
    rescale=1./255,              # æ¨™æº–åŒ–è‡³ [0, 1]
    rotation_range=5,            # Â±5Â°
    width_shift_range=0.1,       # Â±10%
    height_shift_range=0.1,      # Â±10%
    zoom_range=0.1,              # Â±10%
    brightness_range=[0.8, 1.2], # äº®åº¦ [0.8, 1.2]
    fill_mode='nearest',
    horizontal_flip=False,       # äº¤é€šæ¨™èªŒä¸ç¿»è½‰
    vertical_flip=False
)

print("âœ… Data Augmentation è¨­å®šå®Œæˆ:")
print("  - rescale: 1./255")
print("  - rotation_range: Â±5Â°")
print("  - width_shift_range: Â±10%")
print("  - height_shift_range: Â±10%")
print("  - zoom_range: Â±10%")
print("  - brightness_range: [0.8, 1.2]")
print("  - horizontal_flip: False")
print("  - vertical_flip: False")

# 5.3 è¦–è¦ºåŒ– Data Augmentation æ•ˆæœ
print("\nç¹ªè£½ Data Augmentation é©—è­‰åœ–...")
sample_img = X_train_aug[0:1]

fig, axes = plt.subplots(2, 5, figsize=(15, 6))
fig.suptitle('Data Augmentation Verification', fontsize=14, fontweight='bold')

# åŸå§‹å½±åƒ
axes[0, 0].imshow(sample_img[0].astype('uint8'))
axes[0, 0].set_title('Original\n[0-255]', fontsize=10, fontweight='bold')
axes[0, 0].axis('off')

# ç”Ÿæˆ 9 å€‹æ“´å¢æ¨£æœ¬
aug_iter = datagen.flow(sample_img, batch_size=1)
for i in range(9):
    aug_img = next(aug_iter)[0]
    row = (i + 1) // 5
    col = (i + 1) % 5
    axes[row, col].imshow(aug_img)
    axes[row, col].set_title(f'Augmented {i+1}\n[0-1]', fontsize=9)
    axes[row, col].axis('off')

    # æª¢æŸ¥æ˜¯å¦æœ‰å…¨é»‘å•é¡Œ
    if aug_img.max() < 0.1:
        axes[row, col].set_title(f'âŒ BLACK!\nmax={aug_img.max():.4f}',
                                fontsize=9, color='red', fontweight='bold')

plt.tight_layout()
plt.show()

print("âœ… Data Augmentation è¦–è¦ºåŒ–å®Œæˆ")
print("   è«‹ç¢ºèªæ“´å¢å¾Œçš„å½±åƒæ­£å¸¸é¡¯ç¤ºï¼ˆéå…¨é»‘ï¼‰")

# ============================================================================
# Section 6: æ¨¡å‹å»ºç«‹å‡½æ•¸
# ============================================================================

print("\n" + "=" * 80)
print("ğŸ—ï¸ Section 6: æ¨¡å‹å»ºç«‹å‡½æ•¸")
print("=" * 80)

def build_baseline_model(input_shape):
    """å»ºç«‹åŸºç¤ CNN æ¨¡å‹ï¼ˆç„¡ Batch Normalizationï¼‰"""
    model = Sequential(name='Baseline_CNN')

    # ç¬¬ä¸€çµ„å·ç©å±¤
    model.add(Conv2D(32, (5, 5), activation='relu', input_shape=input_shape))
    model.add(Conv2D(32, (5, 5), activation='relu'))
    model.add(MaxPool2D((2, 2)))
    model.add(Dropout(0.25))

    # ç¬¬äºŒçµ„å·ç©å±¤
    model.add(Conv2D(64, (3, 3), activation='relu'))
    model.add(Conv2D(64, (3, 3), activation='relu'))
    model.add(MaxPool2D((2, 2)))
    model.add(Dropout(0.25))

    # å…¨é€£æ¥å±¤
    model.add(Flatten())
    model.add(Dense(256, activation='relu'))
    model.add(Dropout(0.5))
    model.add(Dense(43, activation='softmax'))

    model.compile(
        loss='categorical_crossentropy',
        optimizer='adam',
        metrics=['accuracy']
    )

    return model

def build_model_with_bn(input_shape):
    """å»ºç«‹å« Batch Normalization çš„ CNN æ¨¡å‹"""
    model = Sequential(name='CNN_with_BatchNorm')

    # ç¬¬ä¸€çµ„å·ç©å±¤
    model.add(Conv2D(32, (5, 5), activation='relu', input_shape=input_shape))
    model.add(BatchNormalization())
    model.add(Conv2D(32, (5, 5), activation='relu'))
    model.add(BatchNormalization())
    model.add(MaxPool2D((2, 2)))
    model.add(Dropout(0.25))

    # ç¬¬äºŒçµ„å·ç©å±¤
    model.add(Conv2D(64, (3, 3), activation='relu'))
    model.add(BatchNormalization())
    model.add(Conv2D(64, (3, 3), activation='relu'))
    model.add(BatchNormalization())
    model.add(MaxPool2D((2, 2)))
    model.add(Dropout(0.25))

    # å…¨é€£æ¥å±¤
    model.add(Flatten())
    model.add(Dense(256, activation='relu'))
    model.add(Dropout(0.5))
    model.add(Dense(43, activation='softmax'))

    model.compile(
        loss='categorical_crossentropy',
        optimizer='adam',
        metrics=['accuracy']
    )

    return model

print("âœ… æ¨¡å‹å»ºç«‹å‡½æ•¸å®šç¾©å®Œæˆ")
print("  - build_baseline_model(): åŸºç¤ CNNï¼ˆç„¡ BNï¼‰")
print("  - build_model_with_bn(): CNN + Batch Normalization")

# ============================================================================
# Section 7: Early Stopping è¨­å®š
# ============================================================================

print("\n" + "=" * 80)
print("â±ï¸ Section 7: Early Stopping è¨­å®š")
print("=" * 80)

early_stop = EarlyStopping(
    monitor='val_loss',
    patience=5,
    restore_best_weights=True,
    mode='min',
    verbose=1
)

print("âœ… Early Stopping å·²è¨­å®š:")
print("  - monitor: val_loss")
print("  - patience: 5")
print("  - restore_best_weights: True")
print("  - æ‰€æœ‰å¯¦é©—éƒ½å°‡ä½¿ç”¨æ­¤è¨­å®š")

# ============================================================================
# Section 8: å¯¦é©— Baseline - ç„¡æ¨™æº–åŒ–
# ============================================================================

print("\n" + "=" * 80)
print("ğŸš€ Section 8: å¯¦é©— Baseline - ç„¡æ¨™æº–åŒ–")
print("=" * 80)

print("\nã€å¯¦é©—è¨­å®šã€‘")
print("  âœ… è³‡æ–™: åŸå§‹åƒç´ å€¼ [0, 255]ï¼Œç„¡æ¨™æº–åŒ–")
print("  âœ… æ¨¡å‹: åŸºç¤ CNN (ç„¡ Batch Normalization)")
print("  âœ… Early Stopping: patience=5")
print("  âœ… Batch Size: 128")
print("  âœ… Max Epochs: 35")

# å»ºç«‹æ¨¡å‹
model_baseline = build_baseline_model(X_train_baseline.shape[1:])
print("\nğŸ“‹ æ¨¡å‹æ¶æ§‹:")
model_baseline.summary()

# è¨“ç·´
print("\n" + "=" * 60)
print("é–‹å§‹è¨“ç·´...")
print("=" * 60)

start_time_baseline = time.time()

history_baseline = model_baseline.fit(
    X_train_baseline,
    y_train_encoded,
    batch_size=128,
    epochs=35,
    validation_data=(X_test_baseline, y_test_encoded),
    callbacks=[early_stop],
    verbose=1
)

end_time_baseline = time.time()
training_time_baseline = end_time_baseline - start_time_baseline

# æ¸¬è©¦é›†è©•ä¼°
loss_baseline, acc_baseline = model_baseline.evaluate(
    X_test_baseline, y_test_encoded, verbose=0
)

print("\n" + "=" * 60)
print("âœ… å¯¦é©— Baseline è¨“ç·´å®Œæˆ!")
print("=" * 60)
print(f"â±ï¸  ç¸½è¨“ç·´æ™‚é–“: {training_time_baseline:.2f} ç§’ ({training_time_baseline/60:.2f} åˆ†é˜)")
print(f"ğŸ“Š å¯¦éš›è¨“ç·´ epochs: {len(history_baseline.history['accuracy'])}")
print(f"ğŸ“Š æœ€çµ‚è¨“ç·´æº–ç¢ºç‡: {history_baseline.history['accuracy'][-1]*100:.2f}%")
print(f"ğŸ“Š æœ€çµ‚é©—è­‰æº–ç¢ºç‡: {history_baseline.history['val_accuracy'][-1]*100:.2f}%")
print(f"ğŸ“Š æœ€ä½³é©—è­‰æº–ç¢ºç‡: {max(history_baseline.history['val_accuracy'])*100:.2f}% (Epoch {np.argmax(history_baseline.history['val_accuracy'])+1})")
print(f"ğŸ“Š æ¸¬è©¦æº–ç¢ºç‡: {acc_baseline*100:.2f}%")
print(f"ğŸ“Š æ¸¬è©¦æå¤±: {loss_baseline:.4f}")

# ============================================================================
# Section 9: å¯¦é©— A - æ­£è¦åŒ–
# ============================================================================

print("\n" + "=" * 80)
print("ğŸš€ Section 9: å¯¦é©— A - æ­£è¦åŒ– (Min-Max)")
print("=" * 80)

print("\nã€å¯¦é©—è¨­å®šã€‘")
print("  âœ… è³‡æ–™: Min-Max Normalization [0, 1]")
print("  âœ… æ¨¡å‹: åŸºç¤ CNN (ç„¡ Batch Normalization)")
print("  âœ… Early Stopping: patience=5")
print("  âœ… Batch Size: 128")
print("  âœ… Max Epochs: 35")

# å»ºç«‹æ¨¡å‹
model_norm = build_baseline_model(X_train_norm.shape[1:])
print("\nğŸ“‹ æ¨¡å‹æ¶æ§‹:")
model_norm.summary()

# è¨“ç·´
print("\n" + "=" * 60)
print("é–‹å§‹è¨“ç·´...")
print("=" * 60)

start_time_norm = time.time()

history_norm = model_norm.fit(
    X_train_norm,
    y_train_encoded,
    batch_size=128,
    epochs=35,
    validation_data=(X_test_norm, y_test_encoded),
    callbacks=[early_stop],
    verbose=1
)

end_time_norm = time.time()
training_time_norm = end_time_norm - start_time_norm

# æ¸¬è©¦é›†è©•ä¼°
loss_norm, acc_norm = model_norm.evaluate(
    X_test_norm, y_test_encoded, verbose=0
)

print("\n" + "=" * 60)
print("âœ… å¯¦é©— A è¨“ç·´å®Œæˆ!")
print("=" * 60)
print(f"â±ï¸  ç¸½è¨“ç·´æ™‚é–“: {training_time_norm:.2f} ç§’ ({training_time_norm/60:.2f} åˆ†é˜)")
print(f"ğŸ“Š å¯¦éš›è¨“ç·´ epochs: {len(history_norm.history['accuracy'])}")
print(f"ğŸ“Š æœ€çµ‚è¨“ç·´æº–ç¢ºç‡: {history_norm.history['accuracy'][-1]*100:.2f}%")
print(f"ğŸ“Š æœ€çµ‚é©—è­‰æº–ç¢ºç‡: {history_norm.history['val_accuracy'][-1]*100:.2f}%")
print(f"ğŸ“Š æœ€ä½³é©—è­‰æº–ç¢ºç‡: {max(history_norm.history['val_accuracy'])*100:.2f}% (Epoch {np.argmax(history_norm.history['val_accuracy'])+1})")
print(f"ğŸ“Š æ¸¬è©¦æº–ç¢ºç‡: {acc_norm*100:.2f}%")
print(f"ğŸ“Š æ¸¬è©¦æå¤±: {loss_norm:.4f}")

# ============================================================================
# Section 10: å¯¦é©— B - æ­£è¦åŒ– + è³‡æ–™æ“´å¢
# ============================================================================

print("\n" + "=" * 80)
print("ğŸš€ Section 10: å¯¦é©— B - æ­£è¦åŒ– + è³‡æ–™æ“´å¢")
print("=" * 80)

print("\nã€å¯¦é©—è¨­å®šã€‘")
print("  âœ… è³‡æ–™: ImageDataGenerator (rescale=1./255 + augmentation)")
print("  âœ… æ“´å¢: rotation Â±5Â°, shift Â±10%, zoom Â±10%, brightness [0.8, 1.2]")
print("  âœ… æ¨¡å‹: åŸºç¤ CNN (ç„¡ Batch Normalization)")
print("  âœ… Early Stopping: patience=5")
print("  âœ… Batch Size: 128")
print("  âœ… Max Epochs: 35")

# å»ºç«‹æ¨¡å‹
model_aug = build_baseline_model((30, 30, 3))
print("\nğŸ“‹ æ¨¡å‹æ¶æ§‹:")
model_aug.summary()

# è¨“ç·´
print("\n" + "=" * 60)
print("é–‹å§‹è¨“ç·´...")
print("=" * 60)

start_time_aug = time.time()

steps_per_epoch = int(np.ceil(len(X_train_aug) / 128))

history_aug = model_aug.fit(
    datagen.flow(X_train_aug, y_train_encoded, batch_size=128),
    steps_per_epoch=steps_per_epoch,
    epochs=35,
    validation_data=(X_test_aug_norm, y_test_encoded),
    callbacks=[early_stop],
    verbose=1
)

end_time_aug = time.time()
training_time_aug = end_time_aug - start_time_aug

# æ¸¬è©¦é›†è©•ä¼°
loss_aug, acc_aug = model_aug.evaluate(
    X_test_aug_norm, y_test_encoded, verbose=0
)

print("\n" + "=" * 60)
print("âœ… å¯¦é©— B è¨“ç·´å®Œæˆ!")
print("=" * 60)
print(f"â±ï¸  ç¸½è¨“ç·´æ™‚é–“: {training_time_aug:.2f} ç§’ ({training_time_aug/60:.2f} åˆ†é˜)")
print(f"ğŸ“Š å¯¦éš›è¨“ç·´ epochs: {len(history_aug.history['accuracy'])}")
print(f"ğŸ“Š æœ€çµ‚è¨“ç·´æº–ç¢ºç‡: {history_aug.history['accuracy'][-1]*100:.2f}%")
print(f"ğŸ“Š æœ€çµ‚é©—è­‰æº–ç¢ºç‡: {history_aug.history['val_accuracy'][-1]*100:.2f}%")
print(f"ğŸ“Š æœ€ä½³é©—è­‰æº–ç¢ºç‡: {max(history_aug.history['val_accuracy'])*100:.2f}% (Epoch {np.argmax(history_aug.history['val_accuracy'])+1})")
print(f"ğŸ“Š æ¸¬è©¦æº–ç¢ºç‡: {acc_aug*100:.2f}%")
print(f"ğŸ“Š æ¸¬è©¦æå¤±: {loss_aug:.4f}")

# ============================================================================
# Section 11: å¯¦é©— C - æ­£è¦åŒ– + è³‡æ–™æ“´å¢ + Batch Normalization
# ============================================================================

print("\n" + "=" * 80)
print("ğŸš€ Section 11: å¯¦é©— C - æ­£è¦åŒ– + è³‡æ–™æ“´å¢ + Batch Normalization")
print("=" * 80)

print("\nã€å¯¦é©—è¨­å®šã€‘")
print("  âœ… è³‡æ–™: ImageDataGenerator (rescale=1./255 + augmentation)")
print("  âœ… æ“´å¢: rotation Â±5Â°, shift Â±10%, zoom Â±10%, brightness [0.8, 1.2]")
print("  âœ… æ¨¡å‹: CNN + Batch Normalization (4 å±¤ BN)")
print("  âœ… Early Stopping: patience=5")
print("  âœ… Batch Size: 128")
print("  âœ… Max Epochs: 35")

# å»ºç«‹æ¨¡å‹
model_aug_bn = build_model_with_bn((30, 30, 3))
print("\nğŸ“‹ æ¨¡å‹æ¶æ§‹:")
model_aug_bn.summary()

# è¨“ç·´
print("\n" + "=" * 60)
print("é–‹å§‹è¨“ç·´...")
print("=" * 60)

start_time_aug_bn = time.time()

history_aug_bn = model_aug_bn.fit(
    datagen.flow(X_train_aug, y_train_encoded, batch_size=128),
    steps_per_epoch=steps_per_epoch,
    epochs=35,
    validation_data=(X_test_aug_norm, y_test_encoded),
    callbacks=[early_stop],
    verbose=1
)

end_time_aug_bn = time.time()
training_time_aug_bn = end_time_aug_bn - start_time_aug_bn

# æ¸¬è©¦é›†è©•ä¼°
loss_aug_bn, acc_aug_bn = model_aug_bn.evaluate(
    X_test_aug_norm, y_test_encoded, verbose=0
)

print("\n" + "=" * 60)
print("âœ… å¯¦é©— C è¨“ç·´å®Œæˆ!")
print("=" * 60)
print(f"â±ï¸  ç¸½è¨“ç·´æ™‚é–“: {training_time_aug_bn:.2f} ç§’ ({training_time_aug_bn/60:.2f} åˆ†é˜)")
print(f"ğŸ“Š å¯¦éš›è¨“ç·´ epochs: {len(history_aug_bn.history['accuracy'])}")
print(f"ğŸ“Š æœ€çµ‚è¨“ç·´æº–ç¢ºç‡: {history_aug_bn.history['accuracy'][-1]*100:.2f}%")
print(f"ğŸ“Š æœ€çµ‚é©—è­‰æº–ç¢ºç‡: {history_aug_bn.history['val_accuracy'][-1]*100:.2f}%")
print(f"ğŸ“Š æœ€ä½³é©—è­‰æº–ç¢ºç‡: {max(history_aug_bn.history['val_accuracy'])*100:.2f}% (Epoch {np.argmax(history_aug_bn.history['val_accuracy'])+1})")
print(f"ğŸ“Š æ¸¬è©¦æº–ç¢ºç‡: {acc_aug_bn*100:.2f}%")
print(f"ğŸ“Š æ¸¬è©¦æå¤±: {loss_aug_bn:.4f}")

# ============================================================================
# Section 12: è¨“ç·´æ›²ç·šå°æ¯”è¦–è¦ºåŒ–
# ============================================================================

print("\n" + "=" * 80)
print("ğŸ“Š Section 12: è¨“ç·´æ›²ç·šå°æ¯”è¦–è¦ºåŒ–")
print("=" * 80)

fig, axes = plt.subplots(2, 2, figsize=(16, 10))
fig.suptitle('Training Comparison: 4 Experiments', fontsize=16, fontweight='bold')

# å­åœ– 1: è¨“ç·´æº–ç¢ºç‡
epochs_baseline = range(1, len(history_baseline.history['accuracy']) + 1)
epochs_norm = range(1, len(history_norm.history['accuracy']) + 1)
epochs_aug = range(1, len(history_aug.history['accuracy']) + 1)
epochs_aug_bn = range(1, len(history_aug_bn.history['accuracy']) + 1)

axes[0, 0].plot(epochs_baseline, history_baseline.history['accuracy'],
                'b-o', label='Baseline', linewidth=2, markersize=4)
axes[0, 0].plot(epochs_norm, history_norm.history['accuracy'],
                'g-s', label='Normalized', linewidth=2, markersize=4)
axes[0, 0].plot(epochs_aug, history_aug.history['accuracy'],
                'r-^', label='Norm + Aug', linewidth=2, markersize=4)
axes[0, 0].plot(epochs_aug_bn, history_aug_bn.history['accuracy'],
                'm-d', label='Norm + Aug + BN', linewidth=2, markersize=4)
axes[0, 0].set_xlabel('Epoch', fontsize=12, fontweight='bold')
axes[0, 0].set_ylabel('Training Accuracy', fontsize=12, fontweight='bold')
axes[0, 0].set_title('Training Accuracy', fontsize=13, fontweight='bold')
axes[0, 0].legend(loc='lower right', fontsize=10)
axes[0, 0].grid(True, alpha=0.3)

# å­åœ– 2: é©—è­‰æº–ç¢ºç‡
axes[0, 1].plot(epochs_baseline, history_baseline.history['val_accuracy'],
                'b-o', label='Baseline', linewidth=2, markersize=4)
axes[0, 1].plot(epochs_norm, history_norm.history['val_accuracy'],
                'g-s', label='Normalized', linewidth=2, markersize=4)
axes[0, 1].plot(epochs_aug, history_aug.history['val_accuracy'],
                'r-^', label='Norm + Aug', linewidth=2, markersize=4)
axes[0, 1].plot(epochs_aug_bn, history_aug_bn.history['val_accuracy'],
                'm-d', label='Norm + Aug + BN', linewidth=2, markersize=4)
axes[0, 1].set_xlabel('Epoch', fontsize=12, fontweight='bold')
axes[0, 1].set_ylabel('Validation Accuracy', fontsize=12, fontweight='bold')
axes[0, 1].set_title('Validation Accuracy', fontsize=13, fontweight='bold')
axes[0, 1].legend(loc='lower right', fontsize=10)
axes[0, 1].grid(True, alpha=0.3)

# å­åœ– 3: è¨“ç·´æå¤±
axes[1, 0].plot(epochs_baseline, history_baseline.history['loss'],
                'b-o', label='Baseline', linewidth=2, markersize=4)
axes[1, 0].plot(epochs_norm, history_norm.history['loss'],
                'g-s', label='Normalized', linewidth=2, markersize=4)
axes[1, 0].plot(epochs_aug, history_aug.history['loss'],
                'r-^', label='Norm + Aug', linewidth=2, markersize=4)
axes[1, 0].plot(epochs_aug_bn, history_aug_bn.history['loss'],
                'm-d', label='Norm + Aug + BN', linewidth=2, markersize=4)
axes[1, 0].set_xlabel('Epoch', fontsize=12, fontweight='bold')
axes[1, 0].set_ylabel('Training Loss', fontsize=12, fontweight='bold')
axes[1, 0].set_title('Training Loss', fontsize=13, fontweight='bold')
axes[1, 0].legend(loc='upper right', fontsize=10)
axes[1, 0].grid(True, alpha=0.3)

# å­åœ– 4: é©—è­‰æå¤±
axes[1, 1].plot(epochs_baseline, history_baseline.history['val_loss'],
                'b-o', label='Baseline', linewidth=2, markersize=4)
axes[1, 1].plot(epochs_norm, history_norm.history['val_loss'],
                'g-s', label='Normalized', linewidth=2, markersize=4)
axes[1, 1].plot(epochs_aug, history_aug.history['val_loss'],
                'r-^', label='Norm + Aug', linewidth=2, markersize=4)
axes[1, 1].plot(epochs_aug_bn, history_aug_bn.history['val_loss'],
                'm-d', label='Norm + Aug + BN', linewidth=2, markersize=4)
axes[1, 1].set_xlabel('Epoch', fontsize=12, fontweight='bold')
axes[1, 1].set_ylabel('Validation Loss', fontsize=12, fontweight='bold')
axes[1, 1].set_title('Validation Loss', fontsize=13, fontweight='bold')
axes[1, 1].legend(loc='upper right', fontsize=10)
axes[1, 1].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

print("âœ… è¨“ç·´æ›²ç·šå°æ¯”åœ–ç¹ªè£½å®Œæˆ")

# ============================================================================
# Section 13: æ¸¬è©¦é›†æº–ç¢ºç‡å°æ¯”åœ–
# ============================================================================

print("\n" + "=" * 80)
print("ğŸ“Š Section 13: æ¸¬è©¦é›†æº–ç¢ºç‡å°æ¯”åœ–")
print("=" * 80)

fig, ax = plt.subplots(figsize=(12, 6))

experiments = ['Baseline', 'Normalized', 'Norm + Aug', 'Norm + Aug + BN']
accuracies = [acc_baseline * 100, acc_norm * 100, acc_aug * 100, acc_aug_bn * 100]
colors = ['#3498db', '#2ecc71', '#e74c3c', '#9b59b6']

bars = ax.bar(experiments, accuracies, color=colors, edgecolor='black', linewidth=1.5)

# åœ¨æŸ±ç‹€åœ–ä¸Šæ¨™è¨»æ•¸å€¼
for i, (bar, acc) in enumerate(zip(bars, accuracies)):
    height = bar.get_height()
    ax.text(bar.get_x() + bar.get_width()/2., height,
            f'{acc:.2f}%',
            ha='center', va='bottom', fontsize=12, fontweight='bold')

ax.set_ylabel('Test Accuracy (%)', fontsize=12, fontweight='bold')
ax.set_xlabel('Experiment', fontsize=12, fontweight='bold')
ax.set_title('Test Accuracy Comparison', fontsize=14, fontweight='bold')
ax.set_ylim([95, 100])
ax.grid(axis='y', alpha=0.3)

plt.tight_layout()
plt.show()

print("âœ… æ¸¬è©¦é›†æº–ç¢ºç‡å°æ¯”åœ–ç¹ªè£½å®Œæˆ")

# ============================================================================
# Section 14: å®Œæ•´çµæœæ¯”è¼ƒè¡¨
# ============================================================================

print("\n" + "=" * 80)
print("ğŸ“Š Section 14: å››å€‹å¯¦é©—å®Œæ•´æ¯”è¼ƒ")
print("=" * 80)

comparison_df = pd.DataFrame({
    'æŒ‡æ¨™': [
        'è¨“ç·´æ™‚é–“ (åˆ†é˜)',
        'å¯¦éš›è¨“ç·´ Epochs',
        'æœ€çµ‚è¨“ç·´æº–ç¢ºç‡ (%)',
        'æœ€çµ‚é©—è­‰æº–ç¢ºç‡ (%)',
        'æœ€ä½³é©—è­‰æº–ç¢ºç‡ (%)',
        'æ¸¬è©¦æº–ç¢ºç‡ (%)',
        'æ¸¬è©¦æå¤±',
    ],
    'Baseline': [
        f"{training_time_baseline/60:.2f}",
        f"{len(history_baseline.history['accuracy'])}",
        f"{history_baseline.history['accuracy'][-1]*100:.2f}",
        f"{history_baseline.history['val_accuracy'][-1]*100:.2f}",
        f"{max(history_baseline.history['val_accuracy'])*100:.2f}",
        f"{acc_baseline*100:.2f}",
        f"{loss_baseline:.4f}",
    ],
    'å¯¦é©— A (æ­£è¦åŒ–)': [
        f"{training_time_norm/60:.2f}",
        f"{len(history_norm.history['accuracy'])}",
        f"{history_norm.history['accuracy'][-1]*100:.2f}",
        f"{history_norm.history['val_accuracy'][-1]*100:.2f}",
        f"{max(history_norm.history['val_accuracy'])*100:.2f}",
        f"{acc_norm*100:.2f}",
        f"{loss_norm:.4f}",
    ],
    'å¯¦é©— B (æ­£+æ“´å¢)': [
        f"{training_time_aug/60:.2f}",
        f"{len(history_aug.history['accuracy'])}",
        f"{history_aug.history['accuracy'][-1]*100:.2f}",
        f"{history_aug.history['val_accuracy'][-1]*100:.2f}",
        f"{max(history_aug.history['val_accuracy'])*100:.2f}",
        f"{acc_aug*100:.2f}",
        f"{loss_aug:.4f}",
    ],
    'å¯¦é©— C (æ­£+æ“´å¢+BN)': [
        f"{training_time_aug_bn/60:.2f}",
        f"{len(history_aug_bn.history['accuracy'])}",
        f"{history_aug_bn.history['accuracy'][-1]*100:.2f}",
        f"{history_aug_bn.history['val_accuracy'][-1]*100:.2f}",
        f"{max(history_aug_bn.history['val_accuracy'])*100:.2f}",
        f"{acc_aug_bn*100:.2f}",
        f"{loss_aug_bn:.4f}",
    ]
})

print("\n" + "=" * 100)
print(comparison_df.to_string(index=False))
print("=" * 100)

# ============================================================================
# Section 15: è©³ç´°åˆ†æ
# ============================================================================

print("\n" + "=" * 80)
print("ğŸ“ˆ Section 15: è©³ç´°åˆ†æ")
print("=" * 80)

print("\n1ï¸âƒ£ è¨“ç·´æ•ˆç‡åˆ†æ:")
print(f"  Baseline è¨“ç·´æ™‚é–“: {training_time_baseline/60:.2f} åˆ†é˜ ({len(history_baseline.history['accuracy'])} epochs)")
print(f"  å¯¦é©— A è¨“ç·´æ™‚é–“: {training_time_norm/60:.2f} åˆ†é˜ ({len(history_norm.history['accuracy'])} epochs)")
print(f"  å¯¦é©— B è¨“ç·´æ™‚é–“: {training_time_aug/60:.2f} åˆ†é˜ ({len(history_aug.history['accuracy'])} epochs)")
print(f"  å¯¦é©— C è¨“ç·´æ™‚é–“: {training_time_aug_bn/60:.2f} åˆ†é˜ ({len(history_aug_bn.history['accuracy'])} epochs)")
print(f"  æœ€å¿«æ”¶æ–‚: å¯¦é©— {'C' if len(history_aug_bn.history['accuracy']) <= min(len(history_baseline.history['accuracy']), len(history_norm.history['accuracy']), len(history_aug.history['accuracy'])) else 'B'}")

print("\n2ï¸âƒ£ æº–ç¢ºç‡æå‡åˆ†æ:")
print(f"  Baseline â†’ å¯¦é©— A: {(acc_norm - acc_baseline)*100:+.2f}% (æ­£è¦åŒ–æ•ˆæœ)")
print(f"  å¯¦é©— A â†’ å¯¦é©— B: {(acc_aug - acc_norm)*100:+.2f}% (è³‡æ–™æ“´å¢æ•ˆæœ)")
print(f"  å¯¦é©— B â†’ å¯¦é©— C: {(acc_aug_bn - acc_aug)*100:+.2f}% (Batch Normalization æ•ˆæœ)")
print(f"  Baseline â†’ å¯¦é©— C: {(acc_aug_bn - acc_baseline)*100:+.2f}% (ç¸½æå‡)")

print("\n3ï¸âƒ£ æå¤±é™ä½åˆ†æ:")
print(f"  Baseline: {loss_baseline:.4f}")
print(f"  å¯¦é©— A: {loss_norm:.4f} ({(loss_norm - loss_baseline)/loss_baseline*100:+.1f}%)")
print(f"  å¯¦é©— B: {loss_aug:.4f} ({(loss_aug - loss_baseline)/loss_baseline*100:+.1f}%)")
print(f"  å¯¦é©— C: {loss_aug_bn:.4f} ({(loss_aug_bn - loss_baseline)/loss_baseline*100:+.1f}%)")

print("\n4ï¸âƒ£ éæ“¬åˆåˆ†æ:")
train_val_gap_baseline = history_baseline.history['accuracy'][-1] - history_baseline.history['val_accuracy'][-1]
train_val_gap_norm = history_norm.history['accuracy'][-1] - history_norm.history['val_accuracy'][-1]
train_val_gap_aug = history_aug.history['accuracy'][-1] - history_aug.history['val_accuracy'][-1]
train_val_gap_aug_bn = history_aug_bn.history['accuracy'][-1] - history_aug_bn.history['val_accuracy'][-1]

print(f"  Baseline è¨“ç·´/é©—è­‰å·®è·: {train_val_gap_baseline*100:.2f}%")
print(f"  å¯¦é©— A è¨“ç·´/é©—è­‰å·®è·: {train_val_gap_norm*100:.2f}%")
print(f"  å¯¦é©— B è¨“ç·´/é©—è­‰å·®è·: {train_val_gap_aug*100:.2f}%")
print(f"  å¯¦é©— C è¨“ç·´/é©—è­‰å·®è·: {train_val_gap_aug_bn*100:.2f}%")
print(f"  â†’ å·®è·è¶Šå°è¡¨ç¤ºéæ“¬åˆç¨‹åº¦è¶Šä½")

print("\n5ï¸âƒ£ ç¶œåˆè©•åˆ† (æ¸¬è©¦æº–ç¢ºç‡ + è¨“ç·´æ•ˆç‡):")
# è¨ˆç®—ç¶œåˆè©•åˆ†: æº–ç¢ºç‡æ¬Šé‡ 0.7 + æ•ˆç‡æ¬Šé‡ 0.3
max_time = max(training_time_baseline, training_time_norm, training_time_aug, training_time_aug_bn)
score_baseline = acc_baseline * 0.7 + (1 - training_time_baseline/max_time) * 0.3
score_norm = acc_norm * 0.7 + (1 - training_time_norm/max_time) * 0.3
score_aug = acc_aug * 0.7 + (1 - training_time_aug/max_time) * 0.3
score_aug_bn = acc_aug_bn * 0.7 + (1 - training_time_aug_bn/max_time) * 0.3

print(f"  Baseline: {score_baseline:.4f}")
print(f"  å¯¦é©— A: {score_norm:.4f}")
print(f"  å¯¦é©— B: {score_aug:.4f}")
print(f"  å¯¦é©— C: {score_aug_bn:.4f} â­ æœ€ä½³")

# ============================================================================
# Section 16: å„²å­˜æ¨¡å‹èˆ‡çµæœ
# ============================================================================

print("\n" + "=" * 80)
print("ğŸ’¾ Section 16: å„²å­˜æ¨¡å‹èˆ‡çµæœ")
print("=" * 80)

model_save_path = '/content/drive/MyDrive/Colab Notebooks/GTSRB/'

# å„²å­˜æ¨¡å‹
model_baseline.save(os.path.join(model_save_path, 'model_baseline.h5'))
model_norm.save(os.path.join(model_save_path, 'model_normalized.h5'))
model_aug.save(os.path.join(model_save_path, 'model_augmented.h5'))
model_aug_bn.save(os.path.join(model_save_path, 'model_augmented_bn.h5'))

print("âœ… æ¨¡å‹å·²å„²å­˜:")
print("  - model_baseline.h5")
print("  - model_normalized.h5")
print("  - model_augmented.h5")
print("  - model_augmented_bn.h5")

# å„²å­˜è¨“ç·´æ­·å²
with open(os.path.join(model_save_path, 'history_baseline.pkl'), 'wb') as f:
    pickle.dump(history_baseline.history, f)
with open(os.path.join(model_save_path, 'history_normalized.pkl'), 'wb') as f:
    pickle.dump(history_norm.history, f)
with open(os.path.join(model_save_path, 'history_augmented.pkl'), 'wb') as f:
    pickle.dump(history_aug.history, f)
with open(os.path.join(model_save_path, 'history_augmented_bn.pkl'), 'wb') as f:
    pickle.dump(history_aug_bn.history, f)

print("\nâœ… è¨“ç·´æ­·å²å·²å„²å­˜:")
print("  - history_baseline.pkl")
print("  - history_normalized.pkl")
print("  - history_augmented.pkl")
print("  - history_augmented_bn.pkl")

# å„²å­˜æ¯”è¼ƒçµæœ
results_summary = {
    'baseline': {
        'training_time': training_time_baseline,
        'epochs': len(history_baseline.history['accuracy']),
        'test_acc': acc_baseline,
        'test_loss': loss_baseline
    },
    'normalized': {
        'training_time': training_time_norm,
        'epochs': len(history_norm.history['accuracy']),
        'test_acc': acc_norm,
        'test_loss': loss_norm
    },
    'augmented': {
        'training_time': training_time_aug,
        'epochs': len(history_aug.history['accuracy']),
        'test_acc': acc_aug,
        'test_loss': loss_aug
    },
    'augmented_bn': {
        'training_time': training_time_aug_bn,
        'epochs': len(history_aug_bn.history['accuracy']),
        'test_acc': acc_aug_bn,
        'test_loss': loss_aug_bn
    }
}

with open(os.path.join(model_save_path, 'results_summary.pkl'), 'wb') as f:
    pickle.dump(results_summary, f)

# å„²å­˜ç‚º CSV
comparison_df.to_csv(os.path.join(model_save_path, 'comparison_results.csv'),
                     index=False, encoding='utf-8-sig')

print("\nâœ… çµæœæ‘˜è¦å·²å„²å­˜:")
print("  - results_summary.pkl")
print("  - comparison_results.csv")

# ============================================================================
# å®Œæˆ!
# ============================================================================

print("\n" + "=" * 80)
print("ğŸ‰ å››å¯¦é©—å®Œæ•´æ¯”è¼ƒå®Œæˆ!")
print("=" * 80)

print("\nğŸ“Š æœ€çµ‚çµæœæ‘˜è¦:")
print(f"  ğŸ¥‡ æœ€ä½³æ¸¬è©¦æº–ç¢ºç‡: å¯¦é©— {'C' if acc_aug_bn >= max(acc_baseline, acc_norm, acc_aug) else ('B' if acc_aug >= max(acc_baseline, acc_norm) else ('A' if acc_norm > acc_baseline else 'Baseline'))}")
print(f"     æº–ç¢ºç‡: {max(acc_baseline, acc_norm, acc_aug, acc_aug_bn)*100:.2f}%")
print(f"  âš¡ æœ€å¿«æ”¶æ–‚: å¯¦é©— {'C' if len(history_aug_bn.history['accuracy']) <= min(len(history_baseline.history['accuracy']), len(history_norm.history['accuracy']), len(history_aug.history['accuracy'])) else 'B'}")
print(f"     Epochs: {min(len(history_baseline.history['accuracy']), len(history_norm.history['accuracy']), len(history_aug.history['accuracy']), len(history_aug_bn.history['accuracy']))}")
print(f"  ğŸ“ˆ ç¸½æå‡: {(max(acc_baseline, acc_norm, acc_aug, acc_aug_bn) - acc_baseline)*100:.2f}%")

print("\nğŸ’¡ ä¸»è¦ç™¼ç¾:")
print("  1. æ­£è¦åŒ–æ•ˆæœ: æå‡ {:.2f}%".format((acc_norm - acc_baseline)*100))
print("  2. è³‡æ–™æ“´å¢æ•ˆæœ: æå‡ {:.2f}%".format((acc_aug - acc_norm)*100))
print("  3. Batch Normalization æ•ˆæœ: æå‡ {:.2f}%".format((acc_aug_bn - acc_aug)*100))
print("  4. æ‰€æœ‰å¯¦é©—éƒ½ä½¿ç”¨ Early Stopping (patience=5) ç¢ºä¿å…¬å¹³æ¯”è¼ƒ")

print("\næ„Ÿè¬ä½¿ç”¨! ğŸ“")
print("æ‰€æœ‰çµæœå·²å„²å­˜è‡³: " + model_save_path)
print("=" * 80)


# ============================================================================
# GTSRB - å¯¦é©— C å–®ç¨è¨“ç·´ (å„ªåŒ–ç‰ˆ)
# æ­£è¦åŒ– + è³‡æ–™æ“´å¢ + Batch Normalization
# Early Stopping patience = 20 (é‡å° BN è¨“ç·´æ³¢å‹•å„ªåŒ–)
# ============================================================================

print("=" * 80)
print("ğŸ”¬ GTSRB å¯¦é©— C - å„ªåŒ–ç‰ˆå–®ç¨è¨“ç·´")
print("=" * 80)
print("å¯¦é©—è¨­å®š:")
print("  âœ… è³‡æ–™: ImageDataGenerator (rescale + augmentation)")
print("  âœ… æ“´å¢: rotation Â±5Â°, shift Â±10%, zoom Â±10%, brightness [0.8, 1.2]")
print("  âœ… æ¨¡å‹: CNN + Batch Normalization (4 å±¤ BN)")
print("  âœ… Early Stopping: patience=10 (é‡å° BN æ³¢å‹•å„ªåŒ–) â­")
print("  âœ… Batch Size: 128")
print("  âœ… Max Epochs: 50 (å¢åŠ è¨“ç·´æ©Ÿæœƒ)")
print("=" * 80)

# ============================================================================
# Section 1: ç’°å¢ƒè¨­å®š
# ============================================================================

print("\nğŸ“¦ è¼‰å…¥å¥—ä»¶ä¸­...")

# æ›è¼‰ Google Drive
from google.colab import drive
drive.mount('/content/drive')

# åŸºç¤å¥—ä»¶
import numpy as np
import pandas as pd
import tensorflow as tf
import os
import time
import pickle

# å½±åƒè™•ç†
from PIL import Image

# ç¹ªåœ–å¥—ä»¶
import matplotlib.pyplot as plt
import seaborn as sns

# æ¨¡å‹ç›¸é—œ
from sklearn.model_selection import train_test_split
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPool2D, Dense, Flatten, Dropout, BatchNormalization
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.callbacks import EarlyStopping

# è¨­å®šé¡¯ç¤ºé¢¨æ ¼
plt.style.use('seaborn-v0_8-darkgrid')
sns.set_palette("husl")

# è¨­å®šäº‚æ•¸ç¨®å­
np.random.seed(42)
tf.random.set_seed(42)

print("âœ… å¥—ä»¶è¼‰å…¥å®Œæˆ")
print(f"TensorFlow ç‰ˆæœ¬: {tf.__version__}")
print(f"GPU å¯ç”¨: {tf.config.list_physical_devices('GPU')}")

# ============================================================================
# Section 2: è³‡æ–™è¼‰å…¥
# ============================================================================

print("\n" + "=" * 80)
print("ğŸ“‚ Section 2: è³‡æ–™è¼‰å…¥")
print("=" * 80)

# è·¯å¾‘è¨­å®š
save_dir = '/content/drive/MyDrive/Colab Notebooks/GTSRB/'
data_path = os.path.join(save_dir, 'X_data.npy')
label_path = os.path.join(save_dir, 'y_labels.npy')

print("è¼‰å…¥å¿«å–æª”æ¡ˆ...")
data = np.load(data_path)
labels = np.load(label_path)

print(f"âœ… è³‡æ–™è¼‰å…¥å®Œæˆ:")
print(f"   å½±åƒæ•¸é‡: {data.shape[0]:,} å¼µ")
print(f"   å½±åƒå°ºå¯¸: {data.shape[1]}Ã—{data.shape[2]}")
print(f"   åƒç´ å€¼ç¯„åœ: [{data.min()}, {data.max()}]")

# ============================================================================
# Section 3: è³‡æ–™åˆ†å‰²
# ============================================================================

print("\n" + "=" * 80)
print("ğŸ“Š Section 3: è³‡æ–™åˆ†å‰²")
print("=" * 80)

# 80/20 åˆ†å‰²
X_train, X_test, y_train, y_test = train_test_split(
    data, labels, test_size=0.2, random_state=42, stratify=labels
)

print(f"âœ… è³‡æ–™åˆ†å‰²å®Œæˆ:")
print(f"   è¨“ç·´é›†: {X_train.shape[0]:,} å¼µ ({X_train.shape[0]/len(data)*100:.1f}%)")
print(f"   æ¸¬è©¦é›†: {X_test.shape[0]:,} å¼µ ({X_test.shape[0]/len(data)*100:.1f}%)")

# One-Hot Encoding
y_train_encoded = to_categorical(y_train, 43)
y_test_encoded = to_categorical(y_test, 43)

print(f"\nâœ… One-Hot Encoding å®Œæˆ")

# ============================================================================
# Section 4: è³‡æ–™é è™•ç†
# ============================================================================

print("\n" + "=" * 80)
print("ğŸ”§ Section 4: è³‡æ–™é è™•ç†")
print("=" * 80)

# è¨“ç·´é›†ä¿æŒ [0, 255] çµ¦ ImageDataGenerator
X_train_aug = X_train.copy()

# æ¸¬è©¦é›†æ¨™æº–åŒ–
X_test_norm = X_test.astype('float32') / 255.0

print(f"è¨“ç·´é›†ç¯„åœ: [{X_train_aug.min()}, {X_train_aug.max()}] (çµ¦ ImageDataGenerator)")
print(f"æ¸¬è©¦é›†ç¯„åœ: [{X_test_norm.min():.4f}, {X_test_norm.max():.4f}] (å·²æ¨™æº–åŒ–)")

# ============================================================================
# Section 5: Data Augmentation è¨­å®š
# ============================================================================

print("\n" + "=" * 80)
print("ğŸ¨ Section 5: Data Augmentation è¨­å®š")
print("=" * 80)

datagen = ImageDataGenerator(
    rescale=1./255,              # æ¨™æº–åŒ–
    rotation_range=5,            # Â±5Â°
    width_shift_range=0.1,       # Â±10%
    height_shift_range=0.1,      # Â±10%
    zoom_range=0.1,              # Â±10%
    brightness_range=[0.8, 1.2], # äº®åº¦ [0.8, 1.2]
    fill_mode='nearest',
    horizontal_flip=False,
    vertical_flip=False
)

print("âœ… Data Augmentation è¨­å®šå®Œæˆ:")
print("   - rescale: 1./255")
print("   - rotation_range: Â±5Â°")
print("   - width_shift_range: Â±10%")
print("   - height_shift_range: Â±10%")
print("   - zoom_range: Â±10%")
print("   - brightness_range: [0.8, 1.2]")

# è¦–è¦ºåŒ–é©—è­‰
print("\nç¹ªè£½ Data Augmentation é©—è­‰åœ–...")
sample_img = X_train_aug[0:1]

fig, axes = plt.subplots(2, 5, figsize=(15, 6))
fig.suptitle('Data Augmentation Verification', fontsize=14, fontweight='bold')

# åŸå§‹å½±åƒ
axes[0, 0].imshow(sample_img[0].astype('uint8'))
axes[0, 0].set_title('Original\n[0-255]', fontsize=10, fontweight='bold')
axes[0, 0].axis('off')

# æ“´å¢æ¨£æœ¬
aug_iter = datagen.flow(sample_img, batch_size=1)
for i in range(9):
    aug_img = next(aug_iter)[0]
    row = (i + 1) // 5
    col = (i + 1) % 5
    axes[row, col].imshow(aug_img)
    axes[row, col].set_title(f'Augmented {i+1}\n[0-1]', fontsize=9)
    axes[row, col].axis('off')

plt.tight_layout()
plt.show()

print("âœ… Data Augmentation é©—è­‰å®Œæˆ")

# ============================================================================
# Section 6: æ¨¡å‹å»ºç«‹
# ============================================================================

print("\n" + "=" * 80)
print("ğŸ—ï¸ Section 6: æ¨¡å‹å»ºç«‹ (å« Batch Normalization)")
print("=" * 80)

def build_model_with_bn(input_shape):
    """å»ºç«‹å« Batch Normalization çš„ CNN æ¨¡å‹"""
    model = Sequential(name='CNN_with_BatchNorm_Optimized')

    # ç¬¬ä¸€çµ„å·ç©å±¤ + BN
    model.add(Conv2D(32, (5, 5), activation='relu', input_shape=input_shape))
    model.add(BatchNormalization())
    model.add(Conv2D(32, (5, 5), activation='relu'))
    model.add(BatchNormalization())
    model.add(MaxPool2D((2, 2)))
    model.add(Dropout(0.25))

    # ç¬¬äºŒçµ„å·ç©å±¤ + BN
    model.add(Conv2D(64, (3, 3), activation='relu'))
    model.add(BatchNormalization())
    model.add(Conv2D(64, (3, 3), activation='relu'))
    model.add(BatchNormalization())
    model.add(MaxPool2D((2, 2)))
    model.add(Dropout(0.25))

    # å…¨é€£æ¥å±¤
    model.add(Flatten())
    model.add(Dense(256, activation='relu'))
    model.add(Dropout(0.5))
    model.add(Dense(43, activation='softmax'))

    model.compile(
        loss='categorical_crossentropy',
        optimizer='adam',
        metrics=['accuracy']
    )

    return model

# å»ºç«‹æ¨¡å‹
model = build_model_with_bn((30, 30, 3))

print("\nâœ… æ¨¡å‹å»ºç«‹å®Œæˆ")
print("\nğŸ“‹ æ¨¡å‹æ¶æ§‹:")
model.summary()

# ============================================================================
# Section 7: Early Stopping è¨­å®š (å„ªåŒ–ç‰ˆ)
# ============================================================================

print("\n" + "=" * 80)
print("â±ï¸ Section 7: Early Stopping è¨­å®š (å„ªåŒ–ç‰ˆ)")
print("=" * 80)

early_stop = EarlyStopping(
    monitor='val_loss',
    patience=20,  # â­ å¾ 5 > 20
    restore_best_weights=True,
    mode='min',
    verbose=1
)

print("âœ… Early Stopping å·²è¨­å®š:")
print("   - monitor: val_loss")
print("   - patience: 10 â­ (é‡å° BN è¨“ç·´æ³¢å‹•å„ªåŒ–)")
print("   - restore_best_weights: True")
print("\nğŸ’¡ èªªæ˜:")
print("   BN åœ¨è¨“ç·´åˆæœŸæœƒæœ‰è¼ƒå¤§æ³¢å‹•ï¼Œpatience=20 å¯ä»¥:")
print("   1. å®¹å¿æ›´å¤šæ¬¡é©—è­‰æå¤±æœªæ”¹å–„çš„æƒ…æ³")
print("   2. çµ¦äºˆæ¨¡å‹æ›´å¤šæ™‚é–“ç©©å®šæ”¶æ–‚")
print("   3. é¿å…éæ—©åœæ­¢è¨“ç·´")

# ============================================================================
# Section 8: è¨“ç·´æ¨¡å‹
# ============================================================================

print("\n" + "=" * 80)
print("ğŸš€ Section 8: é–‹å§‹è¨“ç·´å¯¦é©— C (å„ªåŒ–ç‰ˆ)")
print("=" * 80)

print("\nã€è¨“ç·´è¨­å®šã€‘")
print("  âœ… è³‡æ–™: ImageDataGenerator (rescale=1./255 + augmentation)")
print("  âœ… æ“´å¢: rotation Â±5Â°, shift Â±10%, zoom Â±10%, brightness [0.8, 1.2]")
print("  âœ… æ¨¡å‹: CNN + 4 å±¤ Batch Normalization")
print("  âœ… Early Stopping: patience=20 â­")
print("  âœ… Batch Size: 128")
print("  âœ… Max Epochs: 50")

print("\n" + "=" * 60)
print("é–‹å§‹è¨“ç·´...")
print("=" * 60)

start_time = time.time()

steps_per_epoch = int(np.ceil(len(X_train_aug) / 128))

history = model.fit(
    datagen.flow(X_train_aug, y_train_encoded, batch_size=128),
    steps_per_epoch=steps_per_epoch,
    epochs=50,  # å¢åŠ åˆ° 50
    validation_data=(X_test_norm, y_test_encoded),
    callbacks=[early_stop],
    verbose=1
)

end_time = time.time()
training_time = end_time - start_time

# ============================================================================
# Section 9: è¨“ç·´çµæœ
# ============================================================================

print("\n" + "=" * 60)
print("âœ… å¯¦é©— C (å„ªåŒ–ç‰ˆ) è¨“ç·´å®Œæˆ!")
print("=" * 60)

# æ¸¬è©¦é›†è©•ä¼°
loss_test, acc_test = model.evaluate(X_test_norm, y_test_encoded, verbose=0)

print(f"â±ï¸  ç¸½è¨“ç·´æ™‚é–“: {training_time:.2f} ç§’ ({training_time/60:.2f} åˆ†é˜)")
print(f"ğŸ“Š å¯¦éš›è¨“ç·´ epochs: {len(history.history['accuracy'])}")
print(f"ğŸ“Š æœ€çµ‚è¨“ç·´æº–ç¢ºç‡: {history.history['accuracy'][-1]*100:.2f}%")
print(f"ğŸ“Š æœ€çµ‚é©—è­‰æº–ç¢ºç‡: {history.history['val_accuracy'][-1]*100:.2f}%")
print(f"ğŸ“Š æœ€ä½³é©—è­‰æº–ç¢ºç‡: {max(history.history['val_accuracy'])*100:.2f}% (Epoch {np.argmax(history.history['val_accuracy'])+1})")
print(f"ğŸ“Š æœ€ä½³é©—è­‰æå¤±: {min(history.history['val_loss']):.4f} (Epoch {np.argmin(history.history['val_loss'])+1})")
print(f"ğŸ“Š æ¸¬è©¦æº–ç¢ºç‡: {acc_test*100:.2f}%")
print(f"ğŸ“Š æ¸¬è©¦æå¤±: {loss_test:.4f}")

# ============================================================================
# Section 10: èˆ‡åŸå§‹å¯¦é©— C æ¯”è¼ƒ
# ============================================================================

print("\n" + "=" * 80)
print("ğŸ“Š Section 10: èˆ‡åŸå§‹å¯¦é©— C æ¯”è¼ƒ")
print("=" * 80)

# åŸå§‹å¯¦é©— C çµæœ (patience=5)
original_epochs = 12
original_time = 36.53
original_test_acc = 99.71
original_test_loss = 0.0102

comparison_df = pd.DataFrame({
    'æŒ‡æ¨™': [
        'è¨“ç·´æ™‚é–“ (åˆ†é˜)',
        'å¯¦éš›è¨“ç·´ Epochs',
        'æœ€ä½³é©—è­‰æº–ç¢ºç‡ (%)',
        'æ¸¬è©¦æº–ç¢ºç‡ (%)',
        'æ¸¬è©¦æå¤±',
    ],
    'åŸå§‹ (patience=5)': [
        f"{original_time:.2f}",
        f"{original_epochs}",
        "99.71",
        f"{original_test_acc:.2f}",
        f"{original_test_loss:.4f}",
    ],
    'å„ªåŒ–ç‰ˆ (patience=10)': [
        f"{training_time/60:.2f}",
        f"{len(history.history['accuracy'])}",
        f"{max(history.history['val_accuracy'])*100:.2f}",
        f"{acc_test*100:.2f}",
        f"{loss_test:.4f}",
    ]
})

print("\n" + "=" * 80)
print(comparison_df.to_string(index=False))
print("=" * 80)

# è¨ˆç®—æ”¹å–„
acc_improvement = acc_test - (original_test_acc / 100)
loss_improvement = ((original_test_loss - loss_test) / original_test_loss) * 100
epochs_increase = len(history.history['accuracy']) - original_epochs
time_increase = (training_time/60) - original_time

print(f"\nğŸ“ˆ æ”¹å–„åˆ†æ:")
print(f"  æ¸¬è©¦æº–ç¢ºç‡: {acc_improvement*100:+.2f}%")
print(f"  æ¸¬è©¦æå¤±: {loss_improvement:+.1f}%")
print(f"  è¨“ç·´ Epochs: +{epochs_increase} å€‹")
print(f"  è¨“ç·´æ™‚é–“: {time_increase:+.2f} åˆ†é˜")

if acc_test > (original_test_acc / 100):
    print(f"\nâœ… å„ªåŒ–æˆåŠŸ! æ¸¬è©¦æº–ç¢ºç‡æå‡è‡³ {acc_test*100:.2f}%")
else:
    print(f"\nâš ï¸ æº–ç¢ºç‡æœªæå‡ï¼Œä½†è¨“ç·´æ›´ç©©å®š (Epochs: {len(history.history['accuracy'])} vs {original_epochs})")

# ============================================================================
# Section 11: è¨“ç·´æ›²ç·šè¦–è¦ºåŒ–
# ============================================================================

print("\n" + "=" * 80)
print("ğŸ“Š Section 11: è¨“ç·´æ›²ç·šè¦–è¦ºåŒ–")
print("=" * 80)

fig, axes = plt.subplots(1, 2, figsize=(14, 5))
fig.suptitle('Experiment C (Optimized) - Training History', fontsize=14, fontweight='bold')

epochs_range = range(1, len(history.history['accuracy']) + 1)

# å­åœ– 1: æº–ç¢ºç‡
axes[0].plot(epochs_range, history.history['accuracy'],
             'b-o', label='Training Accuracy', linewidth=2, markersize=5)
axes[0].plot(epochs_range, history.history['val_accuracy'],
             'r-s', label='Validation Accuracy', linewidth=2, markersize=5)
axes[0].axvline(x=np.argmax(history.history['val_accuracy'])+1,
                color='green', linestyle='--', alpha=0.5, label='Best Epoch')
axes[0].set_xlabel('Epoch', fontsize=12, fontweight='bold')
axes[0].set_ylabel('Accuracy', fontsize=12, fontweight='bold')
axes[0].set_title('Accuracy (patience=10)', fontsize=13, fontweight='bold')
axes[0].legend(loc='lower right', fontsize=11)
axes[0].grid(True, alpha=0.3)

# å­åœ– 2: æå¤±
axes[1].plot(epochs_range, history.history['loss'],
             'b-o', label='Training Loss', linewidth=2, markersize=5)
axes[1].plot(epochs_range, history.history['val_loss'],
             'r-s', label='Validation Loss', linewidth=2, markersize=5)
axes[1].axvline(x=np.argmin(history.history['val_loss'])+1,
                color='green', linestyle='--', alpha=0.5, label='Best Epoch')
axes[1].set_xlabel('Epoch', fontsize=12, fontweight='bold')
axes[1].set_ylabel('Loss', fontsize=12, fontweight='bold')
axes[1].set_title('Loss (patience=10)', fontsize=13, fontweight='bold')
axes[1].legend(loc='upper right', fontsize=11)
axes[1].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

print("âœ… è¨“ç·´æ›²ç·šç¹ªè£½å®Œæˆ")

# ============================================================================
# Section 12: è¨“ç·´ç©©å®šæ€§åˆ†æ
# ============================================================================

print("\n" + "=" * 80)
print("ğŸ“Š Section 12: è¨“ç·´ç©©å®šæ€§åˆ†æ")
print("=" * 80)

# è¨ˆç®—é©—è­‰æå¤±çš„æ³¢å‹•
val_loss_std = np.std(history.history['val_loss'])
val_loss_mean = np.mean(history.history['val_loss'])
val_loss_cv = (val_loss_std / val_loss_mean) * 100  # è®Šç•°ä¿‚æ•¸

print(f"\né©—è­‰æå¤±çµ±è¨ˆ:")
print(f"  å¹³å‡å€¼: {val_loss_mean:.4f}")
print(f"  æ¨™æº–å·®: {val_loss_std:.4f}")
print(f"  è®Šç•°ä¿‚æ•¸: {val_loss_cv:.2f}%")

# æ‰¾å‡ºæå¤±çªç„¶ä¸Šå‡çš„ epochs
val_loss = history.history['val_loss']
spike_epochs = []
for i in range(1, len(val_loss)):
    if val_loss[i] > val_loss[i-1] * 1.5:  # æå¤±å¢åŠ è¶…é 50%
        spike_epochs.append(i+1)

if spike_epochs:
    print(f"\nâš ï¸ æª¢æ¸¬åˆ°æå¤±çªç„¶ä¸Šå‡çš„ Epochs: {spike_epochs}")
    print("   é€™æ˜¯ Batch Normalization è¨“ç·´éç¨‹çš„æ­£å¸¸ç¾è±¡")
else:
    print(f"\nâœ… è¨“ç·´éç¨‹ç©©å®šï¼Œç„¡æ˜é¡¯æå¤±çªå‡")

# éæ“¬åˆåˆ†æ
train_val_gap = history.history['accuracy'][-1] - history.history['val_accuracy'][-1]
print(f"\néæ“¬åˆåˆ†æ:")
print(f"  è¨“ç·´/é©—è­‰æº–ç¢ºç‡å·®è·: {train_val_gap*100:.2f}%")
if abs(train_val_gap) < 0.02:
    print("  âœ… æ¨¡å‹æ³›åŒ–èƒ½åŠ›è‰¯å¥½")
elif train_val_gap > 0.02:
    print("  âš ï¸ è¼•å¾®éæ“¬åˆ")
else:
    print("  âš ï¸ é©—è­‰æº–ç¢ºç‡é«˜æ–¼è¨“ç·´æº–ç¢ºç‡ (å¯èƒ½éœ€è¦æ›´å¤šè¨“ç·´)")

# ============================================================================
# Section 13: å„²å­˜æ¨¡å‹èˆ‡çµæœ
# ============================================================================

print("\n" + "=" * 80)
print("ğŸ’¾ Section 13: å„²å­˜æ¨¡å‹èˆ‡çµæœ")
print("=" * 80)

model_save_path = '/content/drive/MyDrive/Colab Notebooks/GTSRB/'

# å„²å­˜æ¨¡å‹
model_path = os.path.join(model_save_path, 'model_C_optimized_patience10.h5')
model.save(model_path)
print(f"âœ… æ¨¡å‹å·²å„²å­˜: model_C_optimized_patience10.h5")

# å„²å­˜è¨“ç·´æ­·å²
history_path = os.path.join(model_save_path, 'history_C_optimized_patience10.pkl')
with open(history_path, 'wb') as f:
    pickle.dump(history.history, f)
print(f"âœ… è¨“ç·´æ­·å²å·²å„²å­˜: history_C_optimized_patience10.pkl")

# å„²å­˜æ¯”è¼ƒçµæœ
results = {
    'original': {
        'patience': 5,
        'epochs': original_epochs,
        'time_min': original_time,
        'test_acc': original_test_acc,
        'test_loss': original_test_loss
    },
    'optimized': {
        'patience': 10,
        'epochs': len(history.history['accuracy']),
        'time_min': training_time / 60,
        'test_acc': acc_test * 100,
        'test_loss': loss_test,
        'best_val_acc': max(history.history['val_accuracy']) * 100,
        'best_val_loss': min(history.history['val_loss'])
    }
}

results_path = os.path.join(model_save_path, 'comparison_patience_5_vs_10.pkl')
with open(results_path, 'wb') as f:
    pickle.dump(results, f)
print(f"âœ… æ¯”è¼ƒçµæœå·²å„²å­˜: comparison_patience_5_vs_10.pkl")

# ============================================================================
# Section 14: å®Œæˆ!
# ============================================================================

print("\n" + "=" * 80)
print("ğŸ‰ å¯¦é©— C (å„ªåŒ–ç‰ˆ) å®Œæˆ!")
print("=" * 80)

print("\nğŸ“Š æœ€çµ‚çµæœæ‘˜è¦:")
print(f"   è¨“ç·´æ™‚é–“: {training_time/60:.2f} åˆ†é˜")
print(f"   è¨“ç·´ Epochs: {len(history.history['accuracy'])}")
print(f"   æœ€ä½³é©—è­‰æº–ç¢ºç‡: {max(history.history['val_accuracy'])*100:.2f}%")
print(f"   æ¸¬è©¦æº–ç¢ºç‡: {acc_test*100:.2f}%")
print(f"   æ¸¬è©¦æå¤±: {loss_test:.4f}")

print("\nğŸ’¡ é—œéµç™¼ç¾:")
if len(history.history['accuracy']) > original_epochs:
    print(f"   1. patience=10 ä½¿è¨“ç·´å»¶é•·è‡³ {len(history.history['accuracy'])} epochs")
    print(f"   2. ç›¸æ¯”åŸå§‹çš„ {original_epochs} epochsï¼Œå¢åŠ äº† {len(history.history['accuracy']) - original_epochs} epochs")
    print(f"   3. æ¸¬è©¦æº–ç¢ºç‡: {acc_test*100:.2f}% vs åŸå§‹ {original_test_acc:.2f}%")
else:
    print(f"   1. å³ä½¿ patience=10ï¼Œæ¨¡å‹ä»åœ¨ {len(history.history['accuracy'])} epochs æ”¶æ–‚")
    print(f"   2. é€™é¡¯ç¤ºè¨“ç·´å·²é”æœ€ä½³ç‹€æ…‹")

if acc_test > (original_test_acc / 100):
    print(f"\nğŸ¯ çµè«–: patience=10 æˆåŠŸæå‡æ¨¡å‹æ•ˆèƒ½!")
else:
    print(f"\nğŸ¯ çµè«–: patience=10 æä¾›æ›´ç©©å®šçš„è¨“ç·´éç¨‹")

print("\nğŸ“ æ‰€æœ‰çµæœå·²å„²å­˜è‡³: " + model_save_path)
print("=" * 80)


# ============================================================================
# GTSRB - å¯¦é©— C å„ªåŒ–ç‰ˆ Confusion Matrix è©³ç´°åˆ†æ
# æ¸¬è©¦é›†è©•ä¼°èˆ‡æ··æ·†çŸ©é™£è¦–è¦ºåŒ–
# ============================================================================

print("=" * 80)
print("ğŸ“Š å¯¦é©— C å„ªåŒ–ç‰ˆ - Confusion Matrix è©³ç´°åˆ†æ")
print("=" * 80)

# ============================================================================
# Section 1: è¼‰å…¥å¥—ä»¶
# ============================================================================

print("\nğŸ“¦ è¼‰å…¥å¥—ä»¶...")

from google.colab import drive
drive.mount('/content/drive')

import numpy as np
import pandas as pd
import tensorflow as tf
import os

# å½±åƒè™•ç†
from PIL import Image

# è©•ä¼°æŒ‡æ¨™
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# ç¹ªåœ–
import matplotlib.pyplot as plt
import seaborn as sns

print("âœ… å¥—ä»¶è¼‰å…¥å®Œæˆ")

# ============================================================================
# Section 2: è¼‰å…¥å¯¦é©— C å„ªåŒ–ç‰ˆæ¨¡å‹
# ============================================================================

print("\n" + "=" * 80)
print("ğŸ”§ Section 2: è¼‰å…¥å¯¦é©— C å„ªåŒ–ç‰ˆæ¨¡å‹")
print("=" * 80)

model_path = '/content/drive/MyDrive/Colab Notebooks/GTSRB/model_C_optimized_patience10.h5'

if os.path.exists(model_path):
    model = tf.keras.models.load_model(model_path)
    print(f"âœ… æ¨¡å‹è¼‰å…¥æˆåŠŸ: {model_path}")
    print(f"\nğŸ“‹ æ¨¡å‹æ¶æ§‹æ‘˜è¦:")
    model.summary()
else:
    print(f"âŒ æ‰¾ä¸åˆ°æ¨¡å‹æª”æ¡ˆ: {model_path}")
    print("è«‹ç¢ºèªæ¨¡å‹å·²è¨“ç·´ä¸¦å„²å­˜")
    raise FileNotFoundError(f"Model not found: {model_path}")

# ============================================================================
# Section 3: è¼‰å…¥ä¸¦è™•ç†æ¸¬è©¦è³‡æ–™é›†
# ============================================================================

print("\n" + "=" * 80)
print("ğŸ“‚ Section 3: è¼‰å…¥æ¸¬è©¦è³‡æ–™é›†")
print("=" * 80)

# å¿«å–æª”æ¡ˆè·¯å¾‘
cache_path = '/content/drive/MyDrive/Colab Notebooks/GTSRB/archive/X_test_cache.npy'
label_cache_path = '/content/drive/MyDrive/Colab Notebooks/GTSRB/archive/y_test_cache.npy'

# è¼‰å…¥æ¸¬è©¦è³‡æ–™
if os.path.exists(cache_path) and os.path.exists(label_cache_path):
    print("âš¡ è¼‰å…¥å¿«å–çš„æ¸¬è©¦è³‡æ–™...")
    X_test = np.load(cache_path)
    y_test = np.load(label_cache_path)
    print(f"âœ… å¿«å–è¼‰å…¥æˆåŠŸ")
else:
    print("ğŸ§© ç¬¬ä¸€æ¬¡åŸ·è¡Œï¼šè®€å–ä¸¦è™•ç†åœ–ç‰‡ä¸­...")
    test_csv = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/GTSRB/archive/Test.csv')
    y_test = test_csv["ClassId"].values
    imgs = test_csv["Path"].values

    data = []
    for i, img in enumerate(imgs):
        image = Image.open('/content/drive/MyDrive/Colab Notebooks/GTSRB/archive/' + img)
        image = image.resize((30, 30))
        data.append(np.array(image))

        if (i + 1) % 1000 == 0:
            print(f"  å·²è™•ç† {i + 1}/{len(imgs)} å¼µåœ–ç‰‡...")

    X_test = np.array(data)

    # å­˜æˆå¿«å–æª”
    np.save(cache_path, X_test)
    np.save(label_cache_path, y_test)
    print("âœ… å·²å»ºç«‹å¿«å–ï¼Œä¸‹æ¬¡æœƒç›´æ¥è¼‰å…¥")

print(f"\nğŸ“Š æ¸¬è©¦è³‡æ–™å½¢ç‹€:")
print(f"  X_test: {X_test.shape}")
print(f"  y_test: {y_test.shape}")
print(f"  åƒç´ å€¼ç¯„åœ: [{X_test.min()}, {X_test.max()}]")

# ============================================================================
# Section 4: è³‡æ–™é è™•ç†ï¼ˆæ¨™æº–åŒ–ï¼‰
# ============================================================================

print("\n" + "=" * 80)
print("ğŸ”§ Section 4: è³‡æ–™é è™•ç†")
print("=" * 80)

# é‡è¦ï¼šæ¨™æº–åŒ–åˆ° [0, 1]ï¼Œèˆ‡è¨“ç·´æ™‚ä¸€è‡´
X_test_normalized = X_test.astype('float32') / 255.0

print(f"âœ… æ¨™æº–åŒ–å®Œæˆ")
print(f"  æ¨™æº–åŒ–å¾Œç¯„åœ: [{X_test_normalized.min():.4f}, {X_test_normalized.max():.4f}]")

# ============================================================================
# Section 5: æ¨¡å‹é æ¸¬
# ============================================================================

print("\n" + "=" * 80)
print("ğŸš€ Section 5: æ¨¡å‹é æ¸¬")
print("=" * 80)

print("é–‹å§‹é æ¸¬...")
predictions_prob = model.predict(X_test_normalized, verbose=1)
predictions = np.argmax(predictions_prob, axis=-1)

print(f"\nâœ… é æ¸¬å®Œæˆ")
print(f"  é æ¸¬å½¢ç‹€: {predictions.shape}")
print(f"  é æ¸¬é¡åˆ¥ç¯„åœ: [{predictions.min()}, {predictions.max()}]")

# ============================================================================
# Section 6: è¨ˆç®—æ¸¬è©¦æº–ç¢ºç‡
# ============================================================================

print("\n" + "=" * 80)
print("ğŸ“Š Section 6: æ¸¬è©¦æº–ç¢ºç‡")
print("=" * 80)

test_accuracy = accuracy_score(y_test, predictions) * 100

print(f"âœ… æ¸¬è©¦è³‡æ–™æº–ç¢ºç‡: {test_accuracy:.2f}%")

# è¨ˆç®—æ¯å€‹é¡åˆ¥çš„æº–ç¢ºç‡
unique_classes = np.unique(y_test)
class_accuracies = []

for cls in unique_classes:
    mask = y_test == cls
    if mask.sum() > 0:
        cls_acc = (predictions[mask] == cls).sum() / mask.sum() * 100
        class_accuracies.append(cls_acc)
    else:
        class_accuracies.append(0)

print(f"\nå„é¡åˆ¥æº–ç¢ºç‡çµ±è¨ˆ:")
print(f"  å¹³å‡æº–ç¢ºç‡: {np.mean(class_accuracies):.2f}%")
print(f"  æº–ç¢ºç‡ä¸­ä½æ•¸: {np.median(class_accuracies):.2f}%")
print(f"  æœ€é«˜æº–ç¢ºç‡: {np.max(class_accuracies):.2f}%")
print(f"  æœ€ä½æº–ç¢ºç‡: {np.min(class_accuracies):.2f}%")

# ============================================================================
# Section 7: Classification Report
# ============================================================================

print("\n" + "=" * 80)
print("ğŸ“‹ Section 7: Classification Report")
print("=" * 80)

report = classification_report(y_test, predictions, output_dict=True)
report_df = pd.DataFrame(report).transpose()

print("\nå®Œæ•´ Classification Report:")
print("=" * 80)
print(classification_report(y_test, predictions))
print("=" * 80)

# å„²å­˜ç‚º CSV
report_df.to_csv('/content/drive/MyDrive/Colab Notebooks/GTSRB/classification_report_exp_C_optimized.csv',
                 encoding='utf-8-sig')
print("âœ… Classification Report å·²å„²å­˜ç‚º CSV")

# ============================================================================
# Section 8: æ··æ·†çŸ©é™£è¨ˆç®—
# ============================================================================

print("\n" + "=" * 80)
print("ğŸ“Š Section 8: æ··æ·†çŸ©é™£è¨ˆç®—")
print("=" * 80)

cm = confusion_matrix(y_test, predictions)

print(f"æ··æ·†çŸ©é™£å½¢ç‹€: {cm.shape}")
print(f"ç¸½é æ¸¬æ•¸é‡: {cm.sum()}")
print(f"æ­£ç¢ºé æ¸¬æ•¸é‡: {np.trace(cm)}")
print(f"éŒ¯èª¤é æ¸¬æ•¸é‡: {cm.sum() - np.trace(cm)}")

# ============================================================================
# Section 9: è¦–è¦ºåŒ–æ··æ·†çŸ©é™£ï¼ˆå®Œæ•´ç‰ˆï¼‰
# ============================================================================

print("\n" + "=" * 80)
print("ğŸ“Š Section 9: è¦–è¦ºåŒ–æ··æ·†çŸ©é™£")
print("=" * 80)

# 9.1 å®Œæ•´æ··æ·†çŸ©é™£ï¼ˆ43Ã—43ï¼‰
print("\nç¹ªè£½å®Œæ•´æ··æ·†çŸ©é™£ (43Ã—43)...")

plt.figure(figsize=(18, 15))
sns.heatmap(cm, annot=False, cmap="Blues", fmt='g', cbar_kws={'label': 'Count'})
plt.title("Confusion Matrix - Experiment C (Optimized)\nTest Accuracy: {:.2f}%".format(test_accuracy),
          fontsize=16, fontweight='bold', pad=20)
plt.xlabel("Predicted Label", fontsize=13, fontweight='bold')
plt.ylabel("True Label", fontsize=13, fontweight='bold')
plt.tight_layout()
plt.savefig('/content/drive/MyDrive/Colab Notebooks/GTSRB/confusion_matrix_exp_C_optimized_full.png',
            dpi=300, bbox_inches='tight')
plt.show()

print("âœ… å®Œæ•´æ··æ·†çŸ©é™£å·²å„²å­˜")

# 9.2 æ··æ·†çŸ©é™£ï¼ˆå¸¶æ•¸å­—æ¨™è¨»ï¼Œè¼ƒå°å°ºå¯¸ï¼‰
print("\nç¹ªè£½å¸¶æ•¸å­—æ¨™è¨»çš„æ··æ·†çŸ©é™£...")

plt.figure(figsize=(20, 17))
sns.heatmap(cm, annot=True, cmap="Blues", fmt='d', cbar_kws={'label': 'Count'},
            annot_kws={'fontsize': 6})
plt.title("Confusion Matrix with Counts - Experiment C (Optimized)\nTest Accuracy: {:.2f}%".format(test_accuracy),
          fontsize=16, fontweight='bold', pad=20)
plt.xlabel("Predicted Label", fontsize=13, fontweight='bold')
plt.ylabel("True Label", fontsize=13, fontweight='bold')
plt.tight_layout()
plt.savefig('/content/drive/MyDrive/Colab Notebooks/GTSRB/confusion_matrix_exp_C_optimized_annotated.png',
            dpi=300, bbox_inches='tight')
plt.show()

print("âœ… å¸¶æ¨™è¨»æ··æ·†çŸ©é™£å·²å„²å­˜")

# 9.3 æ­£è¦åŒ–æ··æ·†çŸ©é™£ï¼ˆç™¾åˆ†æ¯”ï¼‰
print("\nç¹ªè£½æ­£è¦åŒ–æ··æ·†çŸ©é™£ï¼ˆç™¾åˆ†æ¯”ï¼‰...")

cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]

plt.figure(figsize=(18, 15))
sns.heatmap(cm_normalized, annot=False, cmap="Blues", fmt='.2%',
            cbar_kws={'label': 'Percentage', 'format': '%.0f%%'})
plt.title("Normalized Confusion Matrix (%) - Experiment C (Optimized)\nTest Accuracy: {:.2f}%".format(test_accuracy),
          fontsize=16, fontweight='bold', pad=20)
plt.xlabel("Predicted Label", fontsize=13, fontweight='bold')
plt.ylabel("True Label", fontsize=13, fontweight='bold')
plt.tight_layout()
plt.savefig('/content/drive/MyDrive/Colab Notebooks/GTSRB/confusion_matrix_exp_C_optimized_normalized.png',
            dpi=300, bbox_inches='tight')
plt.show()

print("âœ… æ­£è¦åŒ–æ··æ·†çŸ©é™£å·²å„²å­˜")

# ============================================================================
# Section 10: éŒ¯èª¤åˆ†æ
# ============================================================================

print("\n" + "=" * 80)
print("ğŸ” Section 10: éŒ¯èª¤åˆ†æ")
print("=" * 80)

# æ‰¾å‡ºæ‰€æœ‰éŒ¯èª¤é æ¸¬
error_mask = predictions != y_test
error_indices = np.where(error_mask)[0]
error_count = len(error_indices)

print(f"\nç¸½éŒ¯èª¤æ•¸: {error_count} / {len(y_test)} ({error_count/len(y_test)*100:.2f}%)")

# çµ±è¨ˆæœ€å¸¸è¦‹çš„éŒ¯èª¤
if error_count > 0:
    error_pairs = list(zip(y_test[error_mask], predictions[error_mask]))
    from collections import Counter
    error_counter = Counter(error_pairs)
    most_common_errors = error_counter.most_common(10)

    print(f"\næœ€å¸¸è¦‹çš„ 10 ç¨®éŒ¯èª¤é æ¸¬:")
    print("=" * 60)
    print(f"{'æ’å':<6} {'çœŸå¯¦é¡åˆ¥':<10} {'é æ¸¬é¡åˆ¥':<10} {'éŒ¯èª¤æ¬¡æ•¸':<10}")
    print("=" * 60)
    for rank, ((true_label, pred_label), count) in enumerate(most_common_errors, 1):
        print(f"{rank:<6} {true_label:<10} {pred_label:<10} {count:<10}")
    print("=" * 60)

    # æ‰¾å‡ºæœ€å®¹æ˜“æ··æ·†çš„é¡åˆ¥
    print(f"\næœ€å®¹æ˜“è¢«èª¤åˆ¤çš„é¡åˆ¥ (Top 5):")
    class_errors = {}
    for cls in unique_classes:
        mask = y_test == cls
        if mask.sum() > 0:
            errors = mask.sum() - (predictions[mask] == cls).sum()
            class_errors[cls] = errors

    sorted_errors = sorted(class_errors.items(), key=lambda x: x[1], reverse=True)[:5]
    print("=" * 60)
    print(f"{'é¡åˆ¥':<10} {'éŒ¯èª¤æ•¸':<10} {'è©²é¡åˆ¥ç¸½æ•¸':<15} {'éŒ¯èª¤ç‡':<10}")
    print("=" * 60)
    for cls, err_count in sorted_errors:
        total = (y_test == cls).sum()
        error_rate = err_count / total * 100 if total > 0 else 0
        print(f"{cls:<10} {err_count:<10} {total:<15} {error_rate:.2f}%")
    print("=" * 60)

else:
    print("ğŸ‰ å®Œç¾é æ¸¬ï¼æ²’æœ‰ä»»ä½•éŒ¯èª¤ï¼")

# ============================================================================
# Section 11: é¡åˆ¥æº–ç¢ºç‡è¦–è¦ºåŒ–
# ============================================================================

print("\n" + "=" * 80)
print("ğŸ“Š Section 11: é¡åˆ¥æº–ç¢ºç‡è¦–è¦ºåŒ–")
print("=" * 80)

# é¡åˆ¥åç¨±å°ç…§
class_names = {
    0:'Speed limit (20km/h)', 1:'Speed limit (30km/h)', 2:'Speed limit (50km/h)',
    3:'Speed limit (60km/h)', 4:'Speed limit (70km/h)', 5:'Speed limit (80km/h)',
    6:'End of speed limit (80km/h)', 7:'Speed limit (100km/h)', 8:'Speed limit (120km/h)',
    9:'No passing', 10:'No passing veh over 3.5 tons', 11:'Right-of-way at intersection',
    12:'Priority road', 13:'Yield', 14:'Stop', 15:'No vehicles',
    16:'Veh > 3.5 tons prohibited', 17:'No entry', 18:'General caution',
    19:'Dangerous curve left', 20:'Dangerous curve right', 21:'Double curve',
    22:'Bumpy road', 23:'Slippery road', 24:'Road narrows on the right',
    25:'Road work', 26:'Traffic signals', 27:'Pedestrians', 28:'Children crossing',
    29:'Bicycles crossing', 30:'Beware of ice/snow', 31:'Wild animals crossing',
    32:'End speed + passing limits', 33:'Turn right ahead', 34:'Turn left ahead',
    35:'Ahead only', 36:'Go straight or right', 37:'Go straight or left',
    38:'Keep right', 39:'Keep left', 40:'Roundabout mandatory',
    41:'End of no passing', 42:'End no passing veh > 3.5 tons'
}

# ç¹ªè£½é¡åˆ¥æº–ç¢ºç‡æŸ±ç‹€åœ–
fig, ax = plt.subplots(figsize=(18, 8))

colors = ['green' if acc == 100 else ('orange' if acc >= 99 else 'red') for acc in class_accuracies]
bars = ax.bar(range(43), class_accuracies, color=colors, edgecolor='black', linewidth=0.5)

ax.axhline(y=100, color='green', linestyle='--', linewidth=1, alpha=0.5, label='100%')
ax.axhline(y=99, color='orange', linestyle='--', linewidth=1, alpha=0.5, label='99%')
ax.axhline(y=test_accuracy, color='blue', linestyle='-', linewidth=2, alpha=0.7,
           label=f'Overall: {test_accuracy:.2f}%')

ax.set_xlabel('Class ID', fontsize=12, fontweight='bold')
ax.set_ylabel('Accuracy (%)', fontsize=12, fontweight='bold')
ax.set_title('Per-Class Accuracy - Experiment C (Optimized)', fontsize=14, fontweight='bold')
ax.set_xticks(range(43))
ax.set_ylim([95, 101])
ax.legend(loc='lower right', fontsize=11)
ax.grid(axis='y', alpha=0.3)

plt.tight_layout()
plt.savefig('/content/drive/MyDrive/Colab Notebooks/GTSRB/class_accuracy_exp_C_optimized.png',
            dpi=300, bbox_inches='tight')
plt.show()

print("âœ… é¡åˆ¥æº–ç¢ºç‡åœ–å·²å„²å­˜")

# çµ±è¨ˆé”åˆ° 100% æº–ç¢ºç‡çš„é¡åˆ¥æ•¸
perfect_classes = sum(1 for acc in class_accuracies if acc == 100)
print(f"\né”åˆ° 100% æº–ç¢ºç‡çš„é¡åˆ¥æ•¸: {perfect_classes} / 43 ({perfect_classes/43*100:.1f}%)")

# ============================================================================
# Section 12: å„²å­˜è©³ç´°åˆ†æçµæœ
# ============================================================================

print("\n" + "=" * 80)
print("ğŸ’¾ Section 12: å„²å­˜è©³ç´°åˆ†æçµæœ")
print("=" * 80)

# å»ºç«‹è©³ç´°åˆ†æ DataFrame
analysis_data = []
for cls in range(43):
    mask = y_test == cls
    total = mask.sum()
    correct = (predictions[mask] == cls).sum()
    accuracy = correct / total * 100 if total > 0 else 0

    analysis_data.append({
        'Class ID': cls,
        'Class Name': class_names[cls],
        'Total Samples': total,
        'Correct Predictions': correct,
        'Accuracy (%)': f"{accuracy:.2f}",
    })

analysis_df = pd.DataFrame(analysis_data)
analysis_df.to_csv('/content/drive/MyDrive/Colab Notebooks/GTSRB/class_analysis_exp_C_optimized.csv',
                   index=False, encoding='utf-8-sig')

print("âœ… é¡åˆ¥åˆ†æå·²å„²å­˜ç‚º CSV")

# å„²å­˜æ··æ·†çŸ©é™£ç‚º CSV
cm_df = pd.DataFrame(cm)
cm_df.to_csv('/content/drive/MyDrive/Colab Notebooks/GTSRB/confusion_matrix_exp_C_optimized.csv',
             index=False, encoding='utf-8-sig')

print("âœ… æ··æ·†çŸ©é™£å·²å„²å­˜ç‚º CSV")

# ============================================================================
# Section 13: ç¸½çµ
# ============================================================================

print("\n" + "=" * 80)
print("ğŸ‰ å¯¦é©— C å„ªåŒ–ç‰ˆ - Confusion Matrix åˆ†æå®Œæˆ!")
print("=" * 80)

print(f"\nğŸ“Š æœ€çµ‚çµæœæ‘˜è¦:")
print(f"  æ¸¬è©¦æº–ç¢ºç‡: {test_accuracy:.2f}%")
print(f"  ç¸½æ¸¬è©¦æ¨£æœ¬: {len(y_test):,}")
print(f"  æ­£ç¢ºé æ¸¬: {(predictions == y_test).sum():,}")
print(f"  éŒ¯èª¤é æ¸¬: {error_count:,}")
print(f"  é”åˆ° 100% æº–ç¢ºç‡çš„é¡åˆ¥: {perfect_classes} / 43")

print(f"\nğŸ“ å·²ç”¢ç”Ÿçš„æª”æ¡ˆ:")
print(f"  1. confusion_matrix_exp_C_optimized_full.png (å®Œæ•´æ··æ·†çŸ©é™£)")
print(f"  2. confusion_matrix_exp_C_optimized_annotated.png (å¸¶æ•¸å­—æ¨™è¨»)")
print(f"  3. confusion_matrix_exp_C_optimized_normalized.png (æ­£è¦åŒ–ç™¾åˆ†æ¯”)")
print(f"  4. class_accuracy_exp_C_optimized.png (é¡åˆ¥æº–ç¢ºç‡åœ–)")
print(f"  5. classification_report_exp_C_optimized.csv (åˆ†é¡å ±å‘Š)")
print(f"  6. class_analysis_exp_C_optimized.csv (é¡åˆ¥åˆ†æ)")
print(f"  7. confusion_matrix_exp_C_optimized.csv (æ··æ·†çŸ©é™£æ•¸æ“š)")

print(f"\næ‰€æœ‰çµæœå·²å„²å­˜è‡³: /content/drive/MyDrive/Colab Notebooks/GTSRB/")
print("=" * 80)